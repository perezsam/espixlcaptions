{
  "video_id": "UIFMLK2nj_w",
  "title": "Second Order Optimization - The Math of Intelligence #2",
  "json": [
    {
      "index": 1,
      "start_time": 17020.0,
      "end_time": 20200.0,
      "text": "Hola Mundo, Es Siraj y vamos a hablar sobre la Optimización."
    },
    {
      "index": 2,
      "start_time": 24200.0,
      "end_time": 33600.0,
      "text": "Hay miles de idiomas hablados al rededor del mundo, Cada uno único en su capacidad de representar conceptos y transmitir ideas."
    },
    {
      "index": 3,
      "start_time": 33600.0,
      "end_time": 40560.0,
      "text": "Ninguna persona habla bien todos ellos, pero hay un lenguajes que es compartido  por todos los humanos, independientemente de donde eres, Matematicas."
    },
    {
      "index": 4,
      "start_time": 40560.0,
      "end_time": 51100.0,
      "text": "No importa tu cultura o tu edad, tu posees  la habilidad de entender este lenguaje de números que nos conecta  a todos a través de los continentes y el tiempo."
    },
    {
      "index": 5,
      "start_time": 51100.0,
      "end_time": 55000.0,
      "text": "como todo lenguaje, la fluidez requiere de practica."
    },
    {
      "index": 6,
      "start_time": 55000.0,
      "end_time": 64460.0,
      "text": "pero a diferencia de algún otro lenguaje, Cuanto más fluido te vuelves en matemáticas, más imparable serás en cualquier cosa que quieras hacer en la vida."
    },
    {
      "index": 7,
      "start_time": 64459.99999999999,
      "end_time": 74740.0,
      "text": "Las matemáticas suceden al rededor de nosotros, Hasta cierto punto la mayoría de la gente no se da cuenta. Podemos pensar en todo como un conjunto de variables, como métricas."
    },
    {
      "index": 8,
      "start_time": 74740.0,
      "end_time": 79320.0,
      "text": "y hay una relación existente entre todas esas variables."
    },
    {
      "index": 9,
      "start_time": 79320.0,
      "end_time": 86940.0,
      "text": "En matemáticas, nosotros llamamos esas relaciones como funciones. Esta es nuestra manera de  representar un conjunto de patrones, un mapeo."
    },
    {
      "index": 10,
      "start_time": 86940.0,
      "end_time": 91260.0,
      "text": "una relación entre muchas variables."
    },
    {
      "index": 11,
      "start_time": 91260.0,
      "end_time": 104560.0,
      "text": "No importa que modelo de aprendizaje automático usamos, no importa que conjunto  de datos usemos, la meta del machine learning es de optimizar para un objetivo y al hacerlo estamos aproximando una función."
    },
    {
      "index": 12,
      "start_time": 104560.0,
      "end_time": 111660.0,
      "text": "El proceso  de optimización nos ayuda reiterativamente a descubrir  las funciones ocultas en la profundidad de los datos."
    },
    {
      "index": 13,
      "start_time": 111660.0,
      "end_time": 118660.0,
      "text": "La semana pasada hablamos acerca de una popular técnica de optimización llamada descenso de gradiente (gradient descent)."
    },
    {
      "index": 14,
      "start_time": 118660.0,
      "end_time": 128020.0,
      "text": "esta se puede desglosar en un proceso de 5 pasos. Primero definimos algun modelo de aprendizaje automático con un conjunto de valores de peso iniciales,"
    },
    {
      "index": 15,
      "start_time": 128020.00000000001,
      "end_time": 133940.0,
      "text": "estos actúan como uno de los coeficientes de la función que el modelo representa."
    },
    {
      "index": 16,
      "start_time": 133940.0,
      "end_time": 147380.0,
      "text": "El mapeo entre los datos de entrada y las predicciones de salida. Estos valores son ingenuos, no tenemos idea de lo que realmente deberían ser, estamos tratando de descubrir los óptimos."
    },
    {
      "index": 17,
      "start_time": 147380.0,
      "end_time": 158400.0,
      "text": "Vamos a definir una  función de error y cuando tracemos el gráfico de una relación entre todos los posibles valores de error y todos  los posibles valores  de peso  para nuestra  función,"
    },
    {
      "index": 18,
      "start_time": 158400.0,
      "end_time": 172220.0,
      "text": "veremos que existe un valle, la mínima. Usaremos nuestro error para  ayudarnos"
    },
    {
      "index": 19,
      "start_time": 172220.0,
      "end_time": 181240.0,
      "text": "La gradiente representa el cambio en el error cuando el peso son cambiados  por un pequeño valor de su valor original."
    },
    {
      "index": 20,
      "start_time": 181240.0,
      "end_time": 189480.0,
      "text": "usamos la gradiente para actualizar los valores de nuestro peso en una dirección tal que el error es minimizado."
    },
    {
      "index": 21,
      "start_time": 189480.0,
      "end_time": 199220.0,
      "text": "Acercándose y acercándose cada vez más a los mínimos de la función. pasamos nuestra solución en la dirección negativa de nuestro gradiente repetidamente."
    },
    {
      "index": 22,
      "start_time": 199220.0,
      "end_time": 208580.0,
      "text": "cuando lo alcanzamos,  hemos aprendido los valores óptimos de peso para nuestro modelo, donde nuestro gradiente es igual a cero."
    },
    {
      "index": 23,
      "start_time": 208580.0,
      "end_time": 214420.0,
      "text": "Nuestro modelo será capaz de hacer predicciones de datos de entrada que nunca se han visto antes."
    },
    {
      "index": 24,
      "start_time": 214420.0,
      "end_time": 219840.0,
      "text": "La mayoría de los problemas de optimización pueden resolverse usando el descenso de gradiente y sus variantes."
    },
    {
      "index": 25,
      "start_time": 219840.0,
      "end_time": 231480.0,
      "text": "todos ellos bajan dentro de la  categoría llamada modelos de primer orden de optimización.\nLos llamamos de primer orden porque sólo requieren que calculemos la primera derivada"
    },
    {
      "index": 26,
      "start_time": 231480.0,
      "end_time": 242200.0,
      "text": "Pero hay otra clase de técnicas que no son tan ampliamente utilizados llamados métodos de optimización de segundo orden que nos obligan a calcular la segunda derivada."
    },
    {
      "index": 27,
      "start_time": 242200.0,
      "end_time": 257340.0,
      "text": "la primera derivada no dice si la función es incrementada o decrece en un punto certero, y la segunda derivada nos dice si la primera derivada ha incrementado o decrecido, lo que indica su curvatura."
    },
    {
      "index": 28,
      "start_time": 257339.99999999997,
      "end_time": 270960.0,
      "text": "los métodos de la primer orden nos proveen con una línea que es tangente a un punto en una superficie de error, y los métodos de la segunda orden nos proporciona una superficie cuadrática que besa la curvatura de la superficie de error."
    },
    {
      "index": 29,
      "start_time": 270960.0,
      "end_time": 283260.0,
      "text": "JAJA consigue un cuarto para dos. La ventaja entonces de los métodos de segundo orden es que no ignoran la curvatura de la superficie de error, y en términos de rendimiento escalonado, son mejores."
    },
    {
      "index": 30,
      "start_time": 283260.0,
      "end_time": 290740.0,
      "text": "Veamos una técnica popular de optimización de segundo orden llamada método de Newton que lleva el nombre del tipo que inventó el cálculo."
    },
    {
      "index": 31,
      "start_time": 290740.0,
      "end_time": 294920.0,
      "text": "Su nombre era..."
    },
    {
      "index": 32,
      "start_time": 294920.0,
      "end_time": 304880.0,
      "text": "Hay actualmente dos versiones de el Método de Newton, la primera versión de para buscar las raíces de un polinomio, todos esos puntos donde se cruza el eje (x)"
    },
    {
      "index": 33,
      "start_time": 304880.0,
      "end_time": 313440.0,
      "text": "Así que si arrojo una pelota y  recuerdo su trayectoria, buscando la raíz de la ecuación te diría exactamente en que momento golpeo el suelo."
    },
    {
      "index": 34,
      "start_time": 313440.0,
      "end_time": 318260.0,
      "text": "La segunda versión es para la optimización y es la cual se usa en el aprendizaje automático."
    },
    {
      "index": 35,
      "start_time": 318260.0,
      "end_time": 324620.0,
      "text": "Pero permite codificar la raíz encontrando la versión primero para desarrollar alguna intuición básica."
    },
    {
      "index": 36,
      "start_time": 358300.0,
      "end_time": 374540.0,
      "text": "Digamos que tenemos una función (f) de (x)(y) una solución inicial estimada. El método de Newton dice que primero encontramos la pendiente de la línea tangente en nuestro punto de conjetura, luego encontramos el punto en el cual la línea tangente que cruza el eje x."
    },
    {
      "index": 37,
      "start_time": 374540.0,
      "end_time": 379340.0,
      "text": "Usaremos ese punto para buscar esta proyección en la función original."
    },
    {
      "index": 38,
      "start_time": 379340.0,
      "end_time": 385600.0,
      "text": "entonces repetimos  otra vez desde nuestro primer paso, esta vez reemplazando nuestro primer punto con este"
    },
    {
      "index": 39,
      "start_time": 385600.0,
      "end_time": 392580.0,
      "text": "nos mantendremos repitiendo y eventualmente pararemos cuando nuestro valor actual de (x) sea menor  o igual que nuestro umbral."
    },
    {
      "index": 40,
      "start_time": 392580.0,
      "end_time": 400480.0,
      "text": "Así que ésa es la raíz que encuentra la versión del Método de Newton, donde estamos tratando de encontrar donde la función es igual a cero,"
    },
    {
      "index": 41,
      "start_time": 400480.0,
      "end_time": 407080.0,
      "text": "Pero en la versión de optimización, estamos tratando de encontrar donde la derivada de la función es igual a cero, sus mínimos."
    },
    {
      "index": 42,
      "start_time": 407080.0,
      "end_time": 420600.0,
      "text": "En un alto nivel, da un aleatorio punto de inicio, construimos una  aproximación cuadrática a la función objetivo que coincide con los valores de primera y segunda derivada en ese punto."
    },
    {
      "index": 43,
      "start_time": 420600.0,
      "end_time": 425860.0,
      "text": "y entonces minimizamos esa función cuadrática en lugar de la función original."
    },
    {
      "index": 44,
      "start_time": 425860.0,
      "end_time": 434440.0,
      "text": "El minimizador de la función cuadrática se utiliza como punto de partida en el siguiente paso y repetimos repetitivamente este proceso."
    },
    {
      "index": 45,
      "start_time": 434440.0,
      "end_time": 441580.0,
      "text": "Ok Vamos a repasar dos casos del Método de Newton para la optimización para aprender más, un caso 1D y un caso 2D."
    },
    {
      "index": 46,
      "start_time": 441600.0,
      "end_time": 452900.0,
      "text": "En el primer caso, tenemos una función uni-dimensional.podemos obtener una aproximación cuadrática en un punto dado de la función usando lo que se llama una expansión de la serie de Taylor"
    },
    {
      "index": 47,
      "start_time": 452900.0,
      "end_time": 456400.0,
      "text": "Descuidando términos de orden tres o más."
    },
    {
      "index": 48,
      "start_time": 457760.0,
      "end_time": 468640.0,
      "text": "Una seria de Taylor es una representación de una función como una infinita suma de términos que son calculados desde los valores de los derivados de funciones en un solo punto."
    },
    {
      "index": 49,
      "start_time": 468660.0,
      "end_time": 485560.0,
      "text": "Fue inventado por un matemático ingles llamado Brook Taylor. Swift (solo bromeo). Así que tomaríamos la serie de Taylor de segundo orden para nuestro punto inicial x, y lo minimizaríamos encontrando la primera derivada y la segunda derivada y comparándolas con cero."
    },
    {
      "index": 50,
      "start_time": 485560.0,
      "end_time": 490320.0,
      "text": "en el  orden para buscar el mínimo valor  de cero, repetimos este proceso."
    },
    {
      "index": 51,
      "start_time": 490320.0,
      "end_time": 500320.0,
      "text": "En el segundo caso, digamos que tenemos una función de múltiples dimensiones. Podemos encontrar el  mínimo de este usando la mismo  enfoque excepto por dos cambios."
    },
    {
      "index": 52,
      "start_time": 500400.0,
      "end_time": 507580.0,
      "text": "remplazamos la primera derivada con una gradiente y la segunda derivada con un hessian."
    },
    {
      "index": 53,
      "start_time": 507580.0,
      "end_time": 516440.0,
      "text": "Un hessian es una matriz de las derivadas parciales de segundo orden de un escalar, y describe la curvatura local de una función multivariable."
    },
    {
      "index": 54,
      "start_time": 516440.00000000006,
      "end_time": 525260.0,
      "text": "Las derivadas nos ayudan a calcular los gradientes que podemos representar usando una matriz Jacobiana para la optimización de primer orden."
    },
    {
      "index": 55,
      "start_time": 525260.0,
      "end_time": 539680.0,
      "text": "Y utilizamos el hessian para la optimización del segundo orden. Estos son 4 de los 5 operadores derivados utilizados en todo el cálculos, son las formas en que organizamos y representamos el cambio numéricamente."
    },
    {
      "index": 56,
      "start_time": 539680.0,
      "end_time": 552780.0,
      "text": "Entonces, ¿cuándo debe usar un método de segundo orden? Los métodos de primer orden suelen ser menos costosos computacionalmente y menos costosos, convirtiendo bastante rápido en grandes conjuntos de datos."
    },
    {
      "index": 57,
      "start_time": 552780.0,
      "end_time": 558900.0,
      "text": "Los métodos de segundo orden son más rápidos cuando se conoce la segunda derivada y es fácil de calcular."
    },
    {
      "index": 58,
      "start_time": 558900.0,
      "end_time": 564740.0,
      "text": "Pero la segunda derivada es a menudo intratable para computar, requiriendo gran cantidad de cálculo."
    },
    {
      "index": 59,
      "start_time": 564740.0,
      "end_time": 573920.0,
      "text": "Para ciertos problemas el descenso del gradiente puede quedar atrapado a lo largo de caminos de convergencia lenta alrededor de los puntos de la silla, mientras que los métodos de segundo orden no lo harán."
    },
    {
      "index": 60,
      "start_time": 573920.0,
      "end_time": 582420.0,
      "text": "Tratar diferentes técnicas de optimización para su problema específico es la mejor manera de ver qué funciona mejor."
    },
    {
      "index": 61,
      "start_time": 582420.0,
      "end_time": 594180.0,
      "text": "Aquí están los puntos clave a recordar: Las técnicas de optimización de primer orden utilizan la primera derivada de una función para minimizarla, las técnicas de optimización de segundo orden utilizadas"
    },
    {
      "index": 62,
      "start_time": 594180.0,
      "end_time": 604260.0,
      "text": "Aquí están los puntos clave a recordar: Las técnicas de optimización de primer orden utilizan la primera derivada de una función para minimizarla, las técnicas de optimización de segundo orden utilizadas"
    },
    {
      "index": 63,
      "start_time": 604260.0,
      "end_time": 616540.0,
      "text": "Y el Método de Newton es una técnica popular de optimización de segundo orden que a veces puede superar el descenso gradiente. Las últimas semanas el ganador del desafío de codificación es Alberto Garcés."
    },
    {
      "index": 64,
      "start_time": 617100.0,
      "end_time": 626440.0,
      "text": "Alberto usó pendiente de gradiente para encontrar la línea de mejor ajuste Su cuaderno de Jupyter es locamente detallado, usted podría aprender el descenso gradiente sólo de leerlo solo."
    },
    {
      "index": 65,
      "start_time": 626440.0,
      "end_time": 631320.0,
      "text": "Muy bien pensado. Eso fue droga Alberto, mago de la semana."
    },
    {
      "index": 66,
      "start_time": 631320.0,
      "end_time": 637100.0,
      "text": "Y el subcampeón es Ivan Gusev que implementó el descenso gradiente desde cero para polinomios de cualquier orden."
    },
    {
      "index": 67,
      "start_time": 637100.0,
      "end_time": 643460.0,
      "text": "EL desafío de la semana es implementar el método de Newton para la optimización desde cero. Detalles en el README,"
    },
    {
      "index": 68,
      "start_time": 643460.0,
      "end_time": 647320.0,
      "text": "Publicar su enlace GitHub en los comentarios, los ganadores anunciados la próxima semana."
    },
    {
      "index": 69,
      "start_time": 647320.0,
      "end_time": 653780.0,
      "text": "Suscríbete para más videos de programación y por ahora tengo que inventar la derivada sexta así que gracias por mirar :)"
    }
  ]
}