{
  "video_id": "pRmcCVoI_Yg",
  "title": "Deep Net Performance - Ep. 24 (Deep Learning SIMPLIFIED)",
  "es": 0,
  "json": [
    {
      "index": 1,
      "start_time": 3389.0,
      "end_time": 6680.0,
      "text": "If you’re familiar with Deep Learning, then I’m sure you’ve heard a lot of talk about"
    },
    {
      "index": 2,
      "start_time": 6680.0,
      "end_time": 13500.0,
      "text": "the importance of GPUs. GPUs are a powerful tool for training deep nets, and nearly every"
    },
    {
      "index": 3,
      "start_time": 13500.0,
      "end_time": 18800.0,
      "text": "software library supports them. But when it comes to speeding up the training process,"
    },
    {
      "index": 4,
      "start_time": 18800.0,
      "end_time": 23849.0,
      "text": "there are several alternatives to GPUs that are worth considering. Let’s take a closer"
    },
    {
      "index": 5,
      "start_time": 23849.0,
      "end_time": 28320.0,
      "text": "look."
    },
    {
      "index": 6,
      "start_time": 28320.0,
      "end_time": 33550.0,
      "text": "The CPU in your computer is capable of performing many different tasks across a wide variety"
    },
    {
      "index": 7,
      "start_time": 33550.0,
      "end_time": 39940.0,
      "text": "of domains. But this versatility comes at a cost – CPUs require sophisticated control"
    },
    {
      "index": 8,
      "start_time": 39940.0,
      "end_time": 45430.0,
      "text": "mechanisms in order to manage the flow of tasks. The CPU is also designed to perform"
    },
    {
      "index": 9,
      "start_time": 45430.0,
      "end_time": 51699.0,
      "text": "tasks serially – one after another – rather than in parallel. Parallelism can also be"
    },
    {
      "index": 10,
      "start_time": 51699.0,
      "end_time": 57100.0,
      "text": "achieved by building in a limited number of cores directly into the CPU."
    },
    {
      "index": 11,
      "start_time": 57100.0,
      "end_time": 63109.0,
      "text": "These cores are also versatile, but they need to be created with general-purpose computing in mind. You"
    },
    {
      "index": 12,
      "start_time": 63109.0,
      "end_time": 67650.0,
      "text": "may have also noticed that CPU clock speeds haven’t improved much over the last few"
    },
    {
      "index": 13,
      "start_time": 67650.0,
      "end_time": 73360.0,
      "text": "years, even though there have been some minor improvements with CPU memory. Since training"
    },
    {
      "index": 14,
      "start_time": 73360.0,
      "end_time": 79220.0,
      "text": "a deep net requires so many computational resources, a CPU is impractical for large-scale"
    },
    {
      "index": 15,
      "start_time": 79220.0,
      "end_time": 83780.0,
      "text": "deep nets."
    },
    {
      "index": 16,
      "start_time": 83780.0,
      "end_time": 89299.0,
      "text": "So if a lone CPU isn’t powerful enough for the job, what can we use to train a deep net"
    },
    {
      "index": 17,
      "start_time": 89299.0,
      "end_time": 94920.0,
      "text": "in a reasonable time window? There are a few tricks we can use, one of which is to implement"
    },
    {
      "index": 18,
      "start_time": 94920.0,
      "end_time": 101659.0,
      "text": "deep nets using vectors. Vector algebra – like addition, dot products, and transposes – are"
    },
    {
      "index": 19,
      "start_time": 101659.0,
      "end_time": 108930.0,
      "text": "all operations that can be performed in parallel. Take the dot product for example…Each multiplication"
    },
    {
      "index": 20,
      "start_time": 108930.0,
      "end_time": 114680.0,
      "text": "step can be performed in parallel, and the resulting products can then be added together."
    },
    {
      "index": 21,
      "start_time": 114680.0,
      "end_time": 119509.0,
      "text": "Through the use of a parallel implementation, deep nets can be trained orders of magnitude"
    },
    {
      "index": 22,
      "start_time": 119509.0,
      "end_time": 125789.0,
      "text": "faster. Parallelism implemented at the hardware level is known as parallel processing, and"
    },
    {
      "index": 23,
      "start_time": 125789.0,
      "end_time": 132409.0,
      "text": "parallelism at the software level is parallel programming."
    },
    {
      "index": 24,
      "start_time": 132409.0,
      "end_time": 137230.0,
      "text": "Parallel processing can be broken down into two general categories – shared memory,"
    },
    {
      "index": 25,
      "start_time": 137230.0,
      "end_time": 143010.0,
      "text": "and distributed computing. Let’s start by looking at a few shared memory options."
    },
    {
      "index": 26,
      "start_time": 143010.0,
      "end_time": 149370.0,
      "text": "The first option is the GPU, a popular tool in the world of deep learning. Unlike a CPU,"
    },
    {
      "index": 27,
      "start_time": 149370.0,
      "end_time": 155090.0,
      "text": "where the number of built-in cores is typically in the single or double digits, GPUs implement"
    },
    {
      "index": 28,
      "start_time": 155090.0,
      "end_time": 161920.0,
      "text": "100s and sometimes even 1000s of cores. Each GPU core is versatile, and capable of general-purpose"
    },
    {
      "index": 29,
      "start_time": 161920.0,
      "end_time": 169090.0,
      "text": "parallel computing. Any task that can be implemented in parallel, can be performed on a GPU. With"
    },
    {
      "index": 30,
      "start_time": 169090.0,
      "end_time": 175239.0,
      "text": "regards to deep nets, the most popular application for GPUs is the training process. The Deep"
    },
    {
      "index": 31,
      "start_time": 175239.0,
      "end_time": 179760.0,
      "text": "Learning community provides great support for GPUs through libraries, implementations,"
    },
    {
      "index": 32,
      "start_time": 179760.0,
      "end_time": 187060.0,
      "text": "and a vibrant ecosystem fostered by nVidia. Despite all their advantages, GPUs do come"
    },
    {
      "index": 33,
      "start_time": 187060.0,
      "end_time": 192120.0,
      "text": "with one big drawback. Their versatility and general-purpose design leads to extremely"
    },
    {
      "index": 34,
      "start_time": 192120.0,
      "end_time": 197760.0,
      "text": "high power consumption. This becomes a significant issue for large scale deep nets, like the"
    },
    {
      "index": 35,
      "start_time": 197760.0,
      "end_time": 203540.0,
      "text": "ones that are used by the tech giants."
    },
    {
      "index": 36,
      "start_time": 203540.0,
      "end_time": 209660.0,
      "text": "One alternative to the GPU is the “Field Programmable Gate Array”, or FPGA. FPGAs"
    },
    {
      "index": 37,
      "start_time": 209660.0,
      "end_time": 215650.0,
      "text": "are highly configurable, and they were originally used by electrical engineers to build mock-ups"
    },
    {
      "index": 38,
      "start_time": 215650.0,
      "end_time": 220450.0,
      "text": "of different chip designs…that way the engineers could test different solutions to a given"
    },
    {
      "index": 39,
      "start_time": 220450.0,
      "end_time": 226319.0,
      "text": "problem, without having to actually design a chip each time. FPGAs allow you to tweak"
    },
    {
      "index": 40,
      "start_time": 226319.0,
      "end_time": 231840.0,
      "text": "the chip’s function at the lowest level, which is the logic gate. So an FPGA can be"
    },
    {
      "index": 41,
      "start_time": 231840.0,
      "end_time": 237659.0,
      "text": "tailored specifically for a deep net application, allowing them to consume much less power than"
    },
    {
      "index": 42,
      "start_time": 237659.0,
      "end_time": 243230.0,
      "text": "a GPU. But there’s an additional benefit, since FPGAs can be used to run a deep net"
    },
    {
      "index": 43,
      "start_time": 243230.0,
      "end_time": 248290.0,
      "text": "model and generate predictions. This would come in handy if, for example, you needed"
    },
    {
      "index": 44,
      "start_time": 248290.0,
      "end_time": 255170.0,
      "text": "to run a large-scale convolutional net across 1000s of images per second. So FPGAs are a"
    },
    {
      "index": 45,
      "start_time": 255170.0,
      "end_time": 260410.0,
      "text": "great tool, but their big strength…that is, their configurability…can also be somewhat"
    },
    {
      "index": 46,
      "start_time": 260410.00000000003,
      "end_time": 267020.0,
      "text": "of a weakness. To properly setup and configure an FPGA, an engineer would need highly-specialized"
    },
    {
      "index": 47,
      "start_time": 267020.0,
      "end_time": 273380.0,
      "text": "knowledge in digital and integrated circuit design."
    },
    {
      "index": 48,
      "start_time": 273380.0,
      "end_time": 279130.0,
      "text": "Another option is an “Application Specific Integrated Circuit”, or ASIC. ASICs are"
    },
    {
      "index": 49,
      "start_time": 279130.0,
      "end_time": 284700.0,
      "text": "highly specialized with designs built in at the hardware and integrated circuit level."
    },
    {
      "index": 50,
      "start_time": 284700.0,
      "end_time": 289250.0,
      "text": "Once built they will perform very well at the task they were designed for but are generally"
    },
    {
      "index": 51,
      "start_time": 289250.0,
      "end_time": 296160.0,
      "text": "unusable at any other tasks. Compared to GPUs and FGPAs, ASICs tend to have the lowest power"
    },
    {
      "index": 52,
      "start_time": 296160.0,
      "end_time": 301540.0,
      "text": "consumption requirements. There are several Deep Learning ASICs such as the Google Tensor"
    },
    {
      "index": 53,
      "start_time": 301540.0,
      "end_time": 311090.0,
      "text": "Processing Unit TPU, and the chip being built by Nervana Systems."
    },
    {
      "index": 54,
      "start_time": 311090.0,
      "end_time": 317310.0,
      "text": "Aside from shared memory, parallelism can also be implemented using distributed computing."
    },
    {
      "index": 55,
      "start_time": 317310.0,
      "end_time": 322440.0,
      "text": "Generally speaking, the three options for distributed computing are data parallelism,"
    },
    {
      "index": 56,
      "start_time": 322440.0,
      "end_time": 328760.0,
      "text": "model parallelism, and pipeline parallelism."
    },
    {
      "index": 57,
      "start_time": 328760.0,
      "end_time": 333490.0,
      "text": "Data parallelism allows you to train different subsets of the data on different nodes in"
    },
    {
      "index": 58,
      "start_time": 333490.0,
      "end_time": 339120.0,
      "text": "a cluster for each training pass. This is followed by parameter averaging and replacement"
    },
    {
      "index": 59,
      "start_time": 339120.0,
      "end_time": 346490.0,
      "text": "across the cluster. We saw model parallelism with TensorFlow, where different portions"
    },
    {
      "index": 60,
      "start_time": 346490.0,
      "end_time": 353010.0,
      "text": "of the model are trained on different devices in parallel."
    },
    {
      "index": 61,
      "start_time": 353010.0,
      "end_time": 357850.0,
      "text": "Pipeline Parallelism works like a production assembly line. Generally, there will be a"
    },
    {
      "index": 62,
      "start_time": 357850.0,
      "end_time": 363280.0,
      "text": "number of jobs to be completed, each of which can be broken up into independent tasks. Each"
    },
    {
      "index": 63,
      "start_time": 363280.0,
      "end_time": 368590.0,
      "text": "task for a given job will be dedicated to a worker, ensuring that each worker is relatively"
    },
    {
      "index": 64,
      "start_time": 368590.0,
      "end_time": 374020.0,
      "text": "well-utilized. When a worker finishes its task, it can move on to a task for another"
    },
    {
      "index": 65,
      "start_time": 374020.0,
      "end_time": 379130.0,
      "text": "job down the line, even if the other workers are still working on the current job. Here"
    },
    {
      "index": 66,
      "start_time": 379130.0,
      "end_time": 384870.0,
      "text": "is an example of a job involving 4 tasks, each of which is dedicated to a worker. When"
    },
    {
      "index": 67,
      "start_time": 384870.0,
      "end_time": 390300.0,
      "text": "worker 1 finishes task 1 for the first job, worker 1 can start working on a task for job"
    },
    {
      "index": 68,
      "start_time": 390300.0,
      "end_time": 396680.0,
      "text": "2. Worker 2 may still be working on task 2 for job 1, and when worker 2 finishes and"
    },
    {
      "index": 69,
      "start_time": 396680.0,
      "end_time": 404270.0,
      "text": "moves to job 2, worker 3 may still be working on task 3 for job 1, and so on. Even though"
    },
    {
      "index": 70,
      "start_time": 404270.0,
      "end_time": 409590.0,
      "text": "this is a bit simplified and processing times can be variable in practice, this example"
    },
    {
      "index": 71,
      "start_time": 409590.0,
      "end_time": 417430.0,
      "text": "should illustrate the concept of pipeline parallelism."
    },
    {
      "index": 72,
      "start_time": 417430.0,
      "end_time": 422000.0,
      "text": "Computer scientists have been researching parallel programming for decades, and in that"
    },
    {
      "index": 73,
      "start_time": 422000.0,
      "end_time": 426490.0,
      "text": "time they’ve developed a set of advanced techniques. Most of these are beyond the scope"
    },
    {
      "index": 74,
      "start_time": 426490.0,
      "end_time": 432000.0,
      "text": "of this video, but the main idea is that designing algorithms with parallelism in mind will allow"
    },
    {
      "index": 75,
      "start_time": 432000.0,
      "end_time": 437130.0,
      "text": "you to take full advantage of the parallelism capabilities of the hardware. Let’s look"
    },
    {
      "index": 76,
      "start_time": 437130.0,
      "end_time": 442270.0,
      "text": "at three general ways to parallelize your code – note that this is an extensive area"
    },
    {
      "index": 77,
      "start_time": 442270.0,
      "end_time": 448740.0,
      "text": "of computer science, so we are not providing an exhaustive list."
    },
    {
      "index": 78,
      "start_time": 448740.0,
      "end_time": 453470.0,
      "text": "The first method is to decompose your data model into chunks, where each chunk is needed"
    },
    {
      "index": 79,
      "start_time": 453470.0,
      "end_time": 458990.0,
      "text": "to perform an instance of a task. In this example, we see a data table where each row"
    },
    {
      "index": 80,
      "start_time": 458990.0,
      "end_time": 464310.0,
      "text": "represents a chunk of data that is independent from the others. By organizing your data in"
    },
    {
      "index": 81,
      "start_time": 464310.0,
      "end_time": 470880.0,
      "text": "this manner, each row can be used as an input in parallel."
    },
    {
      "index": 82,
      "start_time": 470880.0,
      "end_time": 475610.0,
      "text": "The second method is to identify tasks that have dependencies, and place them into a single"
    },
    {
      "index": 83,
      "start_time": 475610.0,
      "end_time": 481260.0,
      "text": "group. By creating multiple groups that have no dependencies on one another, you can process"
    },
    {
      "index": 84,
      "start_time": 481260.0,
      "end_time": 487080.0,
      "text": "the final job in parallel by dividing up the groups."
    },
    {
      "index": 85,
      "start_time": 487080.0,
      "end_time": 491960.0,
      "text": "The third method is to implement threads and processes that handle different tasks or task"
    },
    {
      "index": 86,
      "start_time": 491960.0,
      "end_time": 497750.0,
      "text": "groups. This method can be performed independently, but the performance benefits can be significant"
    },
    {
      "index": 87,
      "start_time": 497750.0,
      "end_time": 500220.0,
      "text": "when combined with the second method."
    },
    {
      "index": 88,
      "start_time": 500220.0,
      "end_time": 505210.0,
      "text": "If you want to learn more about this topic, a great resource is the Open HPI Massive Open"
    },
    {
      "index": 89,
      "start_time": 505210.0,
      "end_time": 512260.0,
      "text": "Online Course on Parallel Programming."
    },
    {
      "index": 90,
      "start_time": 512260.0,
      "end_time": 516640.0,
      "text": "Hopefully by now, you have a better understanding of the available options for training deep"
    },
    {
      "index": 91,
      "start_time": 516640.0,
      "end_time": 521680.0,
      "text": "nets in parallel. Next up, we’ll take a look at the use of deep neural networks for"
    },
    {
      "index": 92,
      "start_time": 521679.99999999994,
      "end_time": 531680.0,
      "text": "Text Analytics."
    }
  ]
}