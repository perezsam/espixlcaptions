{
  "video_id": "VINCQghQRuM",
  "title": "General Sequence Learning using Recurrent Neural Networks",
  "es": 0,
  "json": [
    {
      "index": 1,
      "start_time": 340.0,
      "end_time": 4049.0,
      "text": "Hi everyone, this is Alec and I&#39;m going to be talking to you today about using recurrent"
    },
    {
      "index": 2,
      "start_time": 4049.0000000000005,
      "end_time": 10580.0,
      "text": "neural networks for text analysis. To get an understanding of why this is a potential"
    },
    {
      "index": 3,
      "start_time": 10580.0,
      "end_time": 15670.0,
      "text": "tool to use, it&#39;s good to look at a bit of a history of how text analysis has been done,"
    },
    {
      "index": 4,
      "start_time": 15670.0,
      "end_time": 20720.0,
      "text": "particularly from a machine learning perspective. So in machine learning typically we&#39;re used"
    },
    {
      "index": 5,
      "start_time": 20720.0,
      "end_time": 24199.0,
      "text": "to vector representation so we know how to deal with numbers."
    },
    {
      "index": 6,
      "start_time": 24199.0,
      "end_time": 27630.0,
      "text": "For categories we would use a one HOT vectorization model."
    },
    {
      "index": 7,
      "start_time": 27630.0,
      "end_time": 32320.0,
      "text": "But when we move to trying to understand and classify and regress sequences for instance,"
    },
    {
      "index": 8,
      "start_time": 32320.0,
      "end_time": 37420.0,
      "text": "it becomes much less clear because our tools are typically based on vector approaches."
    },
    {
      "index": 9,
      "start_time": 37420.0,
      "end_time": 42579.0,
      "text": "The way this is typically dealt with is by computing some hard coded feature transformations,"
    },
    {
      "index": 10,
      "start_time": 42579.0,
      "end_time": 49399.0,
      "text": "for instance, using TFIDF vectorizers, some sort of compression model like LSA and then"
    },
    {
      "index": 11,
      "start_time": 49399.0,
      "end_time": 54420.0,
      "text": "plugging a linear model, such as support vector machine or a softmax classifier, on top of"
    },
    {
      "index": 12,
      "start_time": 54420.0,
      "end_time": 57079.0,
      "text": "that. The purpose of this talk today is what happens"
    },
    {
      "index": 13,
      "start_time": 57079.0,
      "end_time": 62280.0,
      "text": "if we cut out those techniques and instead replace them with an RNN."
    },
    {
      "index": 14,
      "start_time": 62280.0,
      "end_time": 66649.0,
      "text": "To get an understanding of why this might be an advantage, structure is hard."
    },
    {
      "index": 15,
      "start_time": 66649.0,
      "end_time": 69549.0,
      "text": "Ngrams are the typical way of preserving some structure."
    },
    {
      "index": 16,
      "start_time": 69549.0,
      "end_time": 73979.0,
      "text": "This would be we take our sentence, for instance, ‘the cat sat on the mat’, and we re-represent"
    },
    {
      "index": 17,
      "start_time": 73979.0,
      "end_time": 78590.0,
      "text": "it as the occurrence of any individual word or combinations of words."
    },
    {
      "index": 18,
      "start_time": 78590.0,
      "end_time": 82170.0,
      "text": "These combinations of words begin to get us the way to see a little bit of structure."
    },
    {
      "index": 19,
      "start_time": 82170.0,
      "end_time": 85880.0,
      "text": "By ‘structure’ I mean preserving the ordering of words."
    },
    {
      "index": 20,
      "start_time": 85880.0,
      "end_time": 92490.0,
      "text": "The problem with this is once we have bigrams or trigrams any combination of two or three"
    },
    {
      "index": 21,
      "start_time": 92490.0,
      "end_time": 97859.0,
      "text": "words quickly becomes huge possibilities. You can have easily 10 million plus features"
    },
    {
      "index": 22,
      "start_time": 97859.0,
      "end_time": 104119.0,
      "text": "and this begins to get cumbersome, require lots of memory and slows things down in and"
    },
    {
      "index": 23,
      "start_time": 104119.0,
      "end_time": 106909.0,
      "text": "of itself. Structure, although it’s difficult, is also"
    },
    {
      "index": 24,
      "start_time": 106909.0,
      "end_time": 111139.0,
      "text": "very important. For certain tasks such as humor or sarcasm,"
    },
    {
      "index": 25,
      "start_time": 111139.0,
      "end_time": 116539.0,
      "text": "looking at a collection of the word ‘cat’ appeared or the word ‘dog’ appeared isn’t"
    },
    {
      "index": 26,
      "start_time": 116539.0,
      "end_time": 119770.0,
      "text": "going to cut it. And that’s what a lot of our models today"
    },
    {
      "index": 27,
      "start_time": 119770.0,
      "end_time": 122739.0,
      "text": "do. To understand though why many models today"
    },
    {
      "index": 28,
      "start_time": 122739.0,
      "end_time": 128039.0,
      "text": "are based on this and do quite successful, ngrams can get you a long way from any task."
    },
    {
      "index": 29,
      "start_time": 128038.99999999999,
      "end_time": 133420.0,
      "text": "Specific words are often very strong indicators: ‘useless’ in the case of negative sentiment"
    },
    {
      "index": 30,
      "start_time": 133420.0,
      "end_time": 134989.0,
      "text": "and ‘fantastic’ in the case of positive sentiment."
    },
    {
      "index": 31,
      "start_time": 134989.0,
      "end_time": 139790.0,
      "text": "If you’re, for instance, trying to classify whether a document is about the stock market"
    },
    {
      "index": 32,
      "start_time": 139790.0,
      "end_time": 145099.0,
      "text": "or is a recipe…you don’t see the word ‘green tea’ come up very much in a stock"
    },
    {
      "index": 33,
      "start_time": 145099.0,
      "end_time": 149629.0,
      "text": "market conversation and you don’t see the word ‘NASDAQ’ come up very much in a recipe."
    },
    {
      "index": 34,
      "start_time": 149629.0,
      "end_time": 153890.0,
      "text": "You can quickly separate things in those kinds of tasks."
    },
    {
      "index": 35,
      "start_time": 153890.0,
      "end_time": 158030.0,
      "text": "It’s often a question of knowing what’s right for your task at hand."
    },
    {
      "index": 36,
      "start_time": 158030.0,
      "end_time": 162780.0,
      "text": "If you’re trying to get at a more qualitative understanding of what’s going on in a body"
    },
    {
      "index": 37,
      "start_time": 162780.0,
      "end_time": 165700.0,
      "text": "of text this is where structure may be very important."
    },
    {
      "index": 38,
      "start_time": 165700.0,
      "end_time": 170859.0,
      "text": "Whereas if you’re just trying to separate out something that may be very indicative"
    },
    {
      "index": 39,
      "start_time": 170859.0,
      "end_time": 179069.0,
      "text": "on a word level, then in many ways a bag of words model can be quite strong."
    },
    {
      "index": 40,
      "start_time": 179069.0,
      "end_time": 182290.0,
      "text": "How an R&amp;N works. To understand its potential advantages over"
    },
    {
      "index": 41,
      "start_time": 182290.0,
      "end_time": 187049.0,
      "text": "a bag of words model what an RNN does is it reads through a sequence iteratively which"
    },
    {
      "index": 42,
      "start_time": 187049.0,
      "end_time": 190230.0,
      "text": "is really nice because it’s how people do it as well."
    },
    {
      "index": 43,
      "start_time": 190230.0,
      "end_time": 193689.0,
      "text": "It’s able to preserve some of the structure of the model."
    },
    {
      "index": 44,
      "start_time": 193689.0,
      "end_time": 199909.0,
      "text": "It goes through each word and updates its hidden representation based on that word and"
    },
    {
      "index": 45,
      "start_time": 199909.0,
      "end_time": 204299.0,
      "text": "the input from the previous hidden state. At time zero where we have no previous hidden"
    },
    {
      "index": 46,
      "start_time": 204299.0,
      "end_time": 210530.0,
      "text": "state we feed in either a bunch of zeroes or we treat it as another parameter to be"
    },
    {
      "index": 47,
      "start_time": 210530.0,
      "end_time": 215150.0,
      "text": "learned in [INAUDIBLE] representation. It just continues to do this all the way through"
    },
    {
      "index": 48,
      "start_time": 215150.0,
      "end_time": 219579.0,
      "text": "the sequence. At each time step we have a 512, in this case"
    },
    {
      "index": 49,
      "start_time": 219579.0,
      "end_time": 223629.0,
      "text": "we had 512 hidden units, dimensional vector representation of our sequence."
    },
    {
      "index": 50,
      "start_time": 223629.0,
      "end_time": 230219.0,
      "text": "It’s a way of taking the sequence of words and using time step converting it into fixed"
    },
    {
      "index": 51,
      "start_time": 230219.0,
      "end_time": 235109.0,
      "text": "length representation. As a bit of notation, arrows would be projections"
    },
    {
      "index": 52,
      "start_time": 235109.0,
      "end_time": 238609.0,
      "text": "dot products and boxes would represent activities."
    },
    {
      "index": 53,
      "start_time": 238609.0,
      "end_time": 242310.0,
      "text": "Vector is the values. For instance, the activation of each hidden"
    },
    {
      "index": 54,
      "start_time": 242310.0,
      "end_time": 246540.0,
      "text": "unit would be this box. It just proceeds iteratively through."
    },
    {
      "index": 55,
      "start_time": 246540.0,
      "end_time": 255150.0,
      "text": "It’s important to note these projections are largely shared across all time sequences."
    },
    {
      "index": 56,
      "start_time": 255150.0,
      "end_time": 261690.0,
      "text": "This projection with this arrow shared for all inputs, across all time steps and this"
    },
    {
      "index": 57,
      "start_time": 261690.0,
      "end_time": 266740.0,
      "text": "hidden to hidden unit connections are preserved as well across the sequence."
    },
    {
      "index": 58,
      "start_time": 266740.0,
      "end_time": 270110.0,
      "text": "This is what makes learning tractable in these models."
    },
    {
      "index": 59,
      "start_time": 270110.0,
      "end_time": 275030.0,
      "text": "At the end of iterating through the sequence we’ve got now a learned representation of"
    },
    {
      "index": 60,
      "start_time": 275030.0,
      "end_time": 279580.0,
      "text": "the sequence, a vector form of our sequence which then can be used by slapping on a traditional"
    },
    {
      "index": 61,
      "start_time": 279580.0,
      "end_time": 283050.0,
      "text": "classifier. In this toy example what we’ve done is we’ve"
    },
    {
      "index": 62,
      "start_time": 283050.0,
      "end_time": 285840.0,
      "text": "read in an input sentence. For instance, we’re trying to teach the"
    },
    {
      "index": 63,
      "start_time": 285840.0,
      "end_time": 291550.0,
      "text": "model how to classify the subject of a sentence. You can also stack them."
    },
    {
      "index": 64,
      "start_time": 291550.0,
      "end_time": 299280.0,
      "text": "Just as your RNN can go through an input sequence and return its internal representation of"
    },
    {
      "index": 65,
      "start_time": 299280.0,
      "end_time": 303020.0,
      "text": "that sequence, you can then train another RNN on top of it or you can jointly train"
    },
    {
      "index": 66,
      "start_time": 303020.0,
      "end_time": 304110.0,
      "text": "both."
    },
    {
      "index": 67,
      "start_time": 304110.0,
      "end_time": 307180.0,
      "text": "The structure is actually quite flexible."
    },
    {
      "index": 68,
      "start_time": 307180.0,
      "end_time": 314080.0,
      "text": "One final note is the way that we do this original input to hidden feed forward is this"
    },
    {
      "index": 69,
      "start_time": 314080.0,
      "end_time": 318750.0,
      "text": "is typically represented either as a traditional one HOT which really doesn’t get us too"
    },
    {
      "index": 70,
      "start_time": 318750.0,
      "end_time": 321830.0,
      "text": "much advantage but what’s really exciting is we can represent this as what&#39;s called"
    },
    {
      "index": 71,
      "start_time": 321830.0,
      "end_time": 323629.0,
      "text": "an ‘embedding matrix’."
    },
    {
      "index": 72,
      "start_time": 323629.0,
      "end_time": 330129.0,
      "text": "These words, ‘the cat sat on the mat’, would be represented as indexes into a matrix."
    },
    {
      "index": 73,
      "start_time": 330129.0,
      "end_time": 334560.0,
      "text": "‘The’ would be represented as index 100."
    },
    {
      "index": 74,
      "start_time": 334560.0,
      "end_time": 341169.0,
      "text": "When we read through the sequence we would look up the row, row 100, and we would return"
    },
    {
      "index": 75,
      "start_time": 341169.0,
      "end_time": 347069.0,
      "text": "as the input being fed into the RNN the learned representation in that embedding matrix."
    },
    {
      "index": 76,
      "start_time": 347069.0,
      "end_time": 353139.0,
      "text": "Let’s say we had 128 dimensions to be learned as an input representation for our words."
    },
    {
      "index": 77,
      "start_time": 353139.0,
      "end_time": 359289.0,
      "text": "It would be equivalent to 128 by let’s say, 10,000 matrix if we do 10,000 words."
    },
    {
      "index": 78,
      "start_time": 359289.0,
      "end_time": 361690.0,
      "text": "We would then feed this in as input."
    },
    {
      "index": 79,
      "start_time": 361690.0,
      "end_time": 364789.0,
      "text": "That’s really cool because we can treat it as a learned w ay to learn representations"
    },
    {
      "index": 80,
      "start_time": 364789.0,
      "end_time": 365830.0,
      "text": "of our words."
    },
    {
      "index": 81,
      "start_time": 365830.0,
      "end_time": 368610.0,
      "text": "We&#39;ll look later in this presentation at what those actually look like."
    },
    {
      "index": 82,
      "start_time": 368610.0,
      "end_time": 374379.0,
      "text": "They give the model a lot of power."
    },
    {
      "index": 83,
      "start_time": 374379.0,
      "end_time": 378930.0,
      "text": "The big thing in the literature is RNNs have a reputation for being very difficult to learn."
    },
    {
      "index": 84,
      "start_time": 378930.0,
      "end_time": 383639.0,
      "text": "They are often known to be unstable in simple art in strange with generic stochastic gradients"
    },
    {
      "index": 85,
      "start_time": 383639.0,
      "end_time": 387229.0,
      "text": "are actually very unstable and difficult to learn."
    },
    {
      "index": 86,
      "start_time": 387229.0,
      "end_time": 390379.0,
      "text": "What has happened in the research literature over the last few years is there are a bunch"
    },
    {
      "index": 87,
      "start_time": 390379.0,
      "end_time": 394789.0,
      "text": "of tricks that have been developed that help them be much more stable, much more powerful"
    },
    {
      "index": 88,
      "start_time": 394789.0,
      "end_time": 397560.0,
      "text": "and much more reliable, effectively."
    },
    {
      "index": 89,
      "start_time": 397560.0,
      "end_time": 403770.0,
      "text": "To get an understanding of these we’re going to go quickly through all these various tricks."
    },
    {
      "index": 90,
      "start_time": 403770.0,
      "end_time": 405940.0,
      "text": "The first of these is gating units."
    },
    {
      "index": 91,
      "start_time": 405940.0,
      "end_time": 409800.0,
      "text": "To understand what a gating unit is we first need to look a little bit more into detail"
    },
    {
      "index": 92,
      "start_time": 409800.0,
      "end_time": 412750.0,
      "text": "how a simple RNN works."
    },
    {
      "index": 93,
      "start_time": 412750.0,
      "end_time": 416699.0,
      "text": "What happens is we have our hidden state from our previous time step."
    },
    {
      "index": 94,
      "start_time": 416699.0,
      "end_time": 421169.0,
      "text": "Again, at the original time step this can just be zeroes or parameters and we receive"
    },
    {
      "index": 95,
      "start_time": 421169.0,
      "end_time": 423389.0,
      "text": "input at time step T."
    },
    {
      "index": 96,
      "start_time": 423389.0,
      "end_time": 429939.0,
      "text": "We take input from the hidden state of h of T minus one and we take input of T."
    },
    {
      "index": 97,
      "start_time": 429939.0,
      "end_time": 436479.0,
      "text": "We just add them together for instance via a dot product projection and then we apply"
    },
    {
      "index": 98,
      "start_time": 436479.0,
      "end_time": 439689.0,
      "text": "an element wise activation function like [INAUDIBLE] for instance."
    },
    {
      "index": 99,
      "start_time": 439689.0,
      "end_time": 441840.0,
      "text": "Then we have a new hidden state."
    },
    {
      "index": 100,
      "start_time": 441840.0,
      "end_time": 445930.0,
      "text": "At the next time step we receive more input, we add it together and apply another element"
    },
    {
      "index": 101,
      "start_time": 445930.0,
      "end_time": 450220.0,
      "text": "wise activation function and this process continues forward."
    },
    {
      "index": 102,
      "start_time": 450220.0,
      "end_time": 454500.0,
      "text": "To understand what the problem that can be with this, is information is always being"
    },
    {
      "index": 103,
      "start_time": 454500.0,
      "end_time": 456650.0,
      "text": "updated at each time step."
    },
    {
      "index": 104,
      "start_time": 456650.0,
      "end_time": 460460.0,
      "text": "As a result it becomes difficult for information to persist through a model like this."
    },
    {
      "index": 105,
      "start_time": 460460.0,
      "end_time": 463099.0,
      "text": "You can think of this as a form of exponential decay."
    },
    {
      "index": 106,
      "start_time": 463099.0,
      "end_time": 469229.0,
      "text": "If we have a value let&#39;s say here, of one and through this process we effectively end"
    },
    {
      "index": 107,
      "start_time": 469229.0,
      "end_time": 475520.0,
      "text": "up multiplying that value by a reasonable .05 what happens after the course of several"
    },
    {
      "index": 108,
      "start_time": 475520.0,
      "end_time": 480509.0,
      "text": "time steps is that value will exponentially decay for instance, to zero."
    },
    {
      "index": 109,
      "start_time": 480509.0,
      "end_time": 483560.0,
      "text": "Information has difficulty spreading through a structure like this."
    },
    {
      "index": 110,
      "start_time": 483560.0,
      "end_time": 489789.0,
      "text": "There have been various changes called ‘gating units’ to make this work better."
    },
    {
      "index": 111,
      "start_time": 489789.0,
      "end_time": 496280.0,
      "text": "What a gating unit does is instead of having the hidden state at a new time step be a direct"
    },
    {
      "index": 112,
      "start_time": 496280.0,
      "end_time": 502520.0,
      "text": "operation of the previous time step, it adds in a variety of gating units that effectively"
    },
    {
      "index": 113,
      "start_time": 502520.0,
      "end_time": 506680.0,
      "text": "transform the information in a more structured way."
    },
    {
      "index": 114,
      "start_time": 506680.0,
      "end_time": 512219.0,
      "text": "One of these is called the ‘gated recurrent unit’ introduced recently."
    },
    {
      "index": 115,
      "start_time": 512219.00000000006,
      "end_time": 514850.00000000006,
      "text": "What it does is it uses two types of gates."
    },
    {
      "index": 116,
      "start_time": 514850.0,
      "end_time": 517390.0,
      "text": "It uses a reset gate and a dynamics gate."
    },
    {
      "index": 117,
      "start_time": 517390.0,
      "end_time": 519910.0,
      "text": "This is the reset gate and this is the dynamics gate."
    },
    {
      "index": 118,
      "start_time": 519909.99999999994,
      "end_time": 524770.0,
      "text": "What the reset gate does is it takes an input from a previous time step, both the hidden"
    },
    {
      "index": 119,
      "start_time": 524770.0,
      "end_time": 530330.0,
      "text": "representation and the input, and it computes a…there should be element wise sigmoid squash"
    },
    {
      "index": 120,
      "start_time": 530330.0,
      "end_time": 535690.0,
      "text": "in here, and what it does is it basically computes how much of the previous time steps’"
    },
    {
      "index": 121,
      "start_time": 535690.0,
      "end_time": 537340.0,
      "text": "information should continue along this route."
    },
    {
      "index": 122,
      "start_time": 537340.0,
      "end_time": 541020.0,
      "text": "A reset value could be anywhere between zero and one and what it does is it multiplies"
    },
    {
      "index": 123,
      "start_time": 541020.0,
      "end_time": 547270.0,
      "text": "the previous hidden states by those reset states."
    },
    {
      "index": 124,
      "start_time": 547270.0,
      "end_time": 551310.0,
      "text": "What this does is it allows a model to adaptively forget information."
    },
    {
      "index": 125,
      "start_time": 551310.0,
      "end_time": 555040.0,
      "text": "For instance, you can imagine for sentiment analysis that some of our information might"
    },
    {
      "index": 126,
      "start_time": 555040.0,
      "end_time": 559940.0,
      "text": "be only relevant on the sentence level and once you see a period your model can then"
    },
    {
      "index": 127,
      "start_time": 559940.0,
      "end_time": 563880.0,
      "text": "clear some of its information because it knows the sentence is over and then be able to use"
    },
    {
      "index": 128,
      "start_time": 563880.0,
      "end_time": 565970.0,
      "text": "it again."
    },
    {
      "index": 129,
      "start_time": 565970.0,
      "end_time": 571480.0,
      "text": "Once we have the previous times states information effectively gated by a reset gate, we then"
    },
    {
      "index": 130,
      "start_time": 571480.0,
      "end_time": 577130.0,
      "text": "update and get our potential new hidden state, h~t."
    },
    {
      "index": 131,
      "start_time": 577130.0,
      "end_time": 583190.0,
      "text": "What we then do is we use the dynamics gate to instead of just using h~t as would be a"
    },
    {
      "index": 132,
      "start_time": 583190.0,
      "end_time": 590260.0,
      "text": "somewhat similar model to the previous example, we average it effectively with the previous"
    },
    {
      "index": 133,
      "start_time": 590260.0,
      "end_time": 592870.0,
      "text": "hidden state based on the dynamics gate."
    },
    {
      "index": 134,
      "start_time": 592870.0,
      "end_time": 600430.0,
      "text": "We take the output of h~t, multiply it by Z again is like our computes values between"
    },
    {
      "index": 135,
      "start_time": 600430.0,
      "end_time": 605850.0,
      "text": "and one for each unit and we multiply it and add together."
    },
    {
      "index": 136,
      "start_time": 605850.0,
      "end_time": 607600.0,
      "text": "This effectively [INAUDIBLE]"
    },
    {
      "index": 137,
      "start_time": 607600.0,
      "end_time": 615930.0,
      "text": "If ‘Z’ is zero what it would do is it would take entirely the new updated h~t and"
    },
    {
      "index": 138,
      "start_time": 615930.0,
      "end_time": 617960.0,
      "text": "none of the previous hidden state."
    },
    {
      "index": 139,
      "start_time": 617960.0,
      "end_time": 623230.0,
      "text": "This would be equivalent in many ways to our previous simple recurrent unit."
    },
    {
      "index": 140,
      "start_time": 623230.0,
      "end_time": 626320.0,
      "text": "In that case we would just have the new value come through."
    },
    {
      "index": 141,
      "start_time": 626320.0,
      "end_time": 630710.0,
      "text": "Sure it would be gated by the recurrent unit but again the new hidden state would be a"
    },
    {
      "index": 142,
      "start_time": 630710.0,
      "end_time": 633920.0,
      "text": "completely new update compared to the previous hidden state."
    },
    {
      "index": 143,
      "start_time": 633920.0,
      "end_time": 640030.0,
      "text": "Whereas if Z is one and this value here is one and this value here is zero so then our"
    },
    {
      "index": 144,
      "start_time": 640030.0,
      "end_time": 644390.0,
      "text": "new hidden state is just a copy effectively of the previous hidden state."
    },
    {
      "index": 145,
      "start_time": 644390.0,
      "end_time": 648500.0,
      "text": "Z values of near one are ways of effectively propagating information over longer steps"
    },
    {
      "index": 146,
      "start_time": 648500.0,
      "end_time": 649830.0,
      "text": "of time."
    },
    {
      "index": 147,
      "start_time": 649830.0,
      "end_time": 654590.0,
      "text": "You can think of it as the easiest way to remember something is just not to change it."
    },
    {
      "index": 148,
      "start_time": 654590.0,
      "end_time": 659390.0,
      "text": "A value can be spread over very long periods of time if we just have Z values near one"
    },
    {
      "index": 149,
      "start_time": 659390.0,
      "end_time": 663500.0,
      "text": "because that way we don&#39;t have all this noise of updating our hidden states."
    },
    {
      "index": 150,
      "start_time": 663500.0,
      "end_time": 665920.0,
      "text": "We just lock it and let that value persist."
    },
    {
      "index": 151,
      "start_time": 665920.0,
      "end_time": 670410.0,
      "text": "That’s a way to, for instance, on the sentence level keep context from the previous sentence"
    },
    {
      "index": 152,
      "start_time": 670410.0,
      "end_time": 674360.0,
      "text": "around if we had Z values that were locked on."
    },
    {
      "index": 153,
      "start_time": 674360.0,
      "end_time": 678080.0,
      "text": "Again, like in our previous model, this can be expanded to another time step just so you"
    },
    {
      "index": 154,
      "start_time": 678080.0,
      "end_time": 679130.0,
      "text": "see how information flows."
    },
    {
      "index": 155,
      "start_time": 679130.0,
      "end_time": 683510.0,
      "text": "Again, there are all these calculations involved in updating our hidden states and our gating"
    },
    {
      "index": 156,
      "start_time": 683510.0,
      "end_time": 688400.0,
      "text": "values but the information at its core really flows through this upper loop of gated values"
    },
    {
      "index": 157,
      "start_time": 688400.0,
      "end_time": 693000.0,
      "text": "interacting with previous hidden states."
    },
    {
      "index": 158,
      "start_time": 693000.0,
      "end_time": 694110.0,
      "text": "Gating is essential."
    },
    {
      "index": 159,
      "start_time": 694110.0,
      "end_time": 699130.0,
      "text": "That was enough of an example of all the theoretical reasons why these better designed gates might"
    },
    {
      "index": 160,
      "start_time": 699130.0,
      "end_time": 701290.0,
      "text": "help propagate information better."
    },
    {
      "index": 161,
      "start_time": 701290.0,
      "end_time": 703790.0,
      "text": "But empirically it&#39;s also very important."
    },
    {
      "index": 162,
      "start_time": 703790.0,
      "end_time": 707230.0,
      "text": "For sentiment analysis of longer sequence of text, for instance, a paragraph or so,"
    },
    {
      "index": 163,
      "start_time": 707230.0,
      "end_time": 711500.0,
      "text": "a few hundred words for instance, a simple RNN has difficulty learning it all."
    },
    {
      "index": 164,
      "start_time": 711500.0,
      "end_time": 715620.0,
      "text": "You can see that it initially climbs downhill a little bit but all it’s actually doing"
    },
    {
      "index": 165,
      "start_time": 715620.0,
      "end_time": 720130.0,
      "text": "here is just predicting the average sentiment, for instance, 0.5."
    },
    {
      "index": 166,
      "start_time": 720130.0,
      "end_time": 724480.0,
      "text": "Whereas a gated unit, a recurrent neural network, is able to quickly learn and continuously"
    },
    {
      "index": 167,
      "start_time": 724480.0,
      "end_time": 725410.0,
      "text": "learn."
    },
    {
      "index": 168,
      "start_time": 725410.0,
      "end_time": 729620.0,
      "text": "Again, you can&#39;t use simple recurrent units for these more complex tasks especially when"
    },
    {
      "index": 169,
      "start_time": 729620.0,
      "end_time": 733430.0,
      "text": "you have longer sequences of 100 plus words or tokens."
    },
    {
      "index": 170,
      "start_time": 733430.0,
      "end_time": 737970.0,
      "text": "They just don&#39;t work well because information is hard to keep over longer sequences of time"
    },
    {
      "index": 171,
      "start_time": 737970.0,
      "end_time": 739890.0,
      "text": "in those kinds of models."
    },
    {
      "index": 172,
      "start_time": 739890.0,
      "end_time": 743460.0,
      "text": "Now that we&#39;ve talked about gating there&#39;s another question which is what kind of gating"
    },
    {
      "index": 173,
      "start_time": 743460.0,
      "end_time": 744230.0,
      "text": "do you use."
    },
    {
      "index": 174,
      "start_time": 744230.0,
      "end_time": 746580.0,
      "text": "There are two types of models that have been proposed."
    },
    {
      "index": 175,
      "start_time": 746580.0,
      "end_time": 751110.0,
      "text": "Gated recurrent units by Cho, recently from the University Montreal, which are used for"
    },
    {
      "index": 176,
      "start_time": 751110.0,
      "end_time": 753990.0,
      "text": "machine translation and speech recognition tasks."
    },
    {
      "index": 177,
      "start_time": 753990.0,
      "end_time": 757610.0,
      "text": "Then there&#39;s also with the more traditional long short term memory."
    },
    {
      "index": 178,
      "start_time": 757610.0,
      "end_time": 760930.0,
      "text": "This has been around much longer and has been used in far more papers."
    },
    {
      "index": 179,
      "start_time": 760930.0,
      "end_time": 766880.0,
      "text": "Various modifications to the classic architecture exist but for text analysis GRU seems to be"
    },
    {
      "index": 180,
      "start_time": 766880.0,
      "end_time": 768160.0,
      "text": "quite nice in general."
    },
    {
      "index": 181,
      "start_time": 768160.0,
      "end_time": 772880.0,
      "text": "It seems to be simpler, faster and optimizes quicker at least on the sentiment analysis"
    },
    {
      "index": 182,
      "start_time": 772880.0,
      "end_time": 773510.0,
      "text": "dataset."
    },
    {
      "index": 183,
      "start_time": 773510.0,
      "end_time": 777890.0,
      "text": "Because it only has two gates compared to LSTM’s four it’s also a little bit faster."
    },
    {
      "index": 184,
      "start_time": 777890.0,
      "end_time": 781660.0,
      "text": "If you have a larger dataset and you don&#39;t mind waiting a little bit longer, LSTM may"
    },
    {
      "index": 185,
      "start_time": 781660.0,
      "end_time": 785730.0,
      "text": "be better in the long run especially with larger datasets because it has additional"
    },
    {
      "index": 186,
      "start_time": 785730.0,
      "end_time": 788470.0,
      "text": "complexity with more gates."
    },
    {
      "index": 187,
      "start_time": 788470.0,
      "end_time": 792740.0,
      "text": "But again it seems like GRU does quite well in these kinds of problems generally."
    },
    {
      "index": 188,
      "start_time": 792740.0,
      "end_time": 797510.0,
      "text": "I tend to favor it myself but you can try both."
    },
    {
      "index": 189,
      "start_time": 797510.0,
      "end_time": 801490.0,
      "text": "The library we’ll be introducing later in this talk supports both."
    },
    {
      "index": 190,
      "start_time": 801490.0,
      "end_time": 804430.0,
      "text": "The next question is exploding gradients."
    },
    {
      "index": 191,
      "start_time": 804430.0,
      "end_time": 807970.0,
      "text": "Exploding gradients are a training dynamics phenomena that happens in recurrent neural"
    },
    {
      "index": 192,
      "start_time": 807970.0,
      "end_time": 813820.0,
      "text": "networks where the values that we’re trying to update [INAUDIBLE] at each step of our"
    },
    {
      "index": 193,
      "start_time": 813820.0,
      "end_time": 817750.0,
      "text": "training algorithm can become very large and very unstable."
    },
    {
      "index": 194,
      "start_time": 817750.0,
      "end_time": 823170.0,
      "text": "This is one of the sources of the reputation of RNNs being hard to train."
    },
    {
      "index": 195,
      "start_time": 823170.0,
      "end_time": 827690.0,
      "text": "Typically you would see small values, for instance the norm of your gradient would be"
    },
    {
      "index": 196,
      "start_time": 827690.0,
      "end_time": 832260.0,
      "text": "around one and just bouncing around and then sometimes you’d see huge spikes."
    },
    {
      "index": 197,
      "start_time": 832260.0,
      "end_time": 837690.0,
      "text": "Those spikes can be quite damaging because a traditional learning update would then rapidly"
    },
    {
      "index": 198,
      "start_time": 837690.0,
      "end_time": 841340.0,
      "text": "change your values and this could result in unstable oscillations and your whole model"
    },
    {
      "index": 199,
      "start_time": 841340.0,
      "end_time": 841750.0,
      "text": "explodes."
    },
    {
      "index": 200,
      "start_time": 841750.0,
      "end_time": 846430.0,
      "text": "In 2012 there was a great paper that proposed simply clipping the norm of the gradient."
    },
    {
      "index": 201,
      "start_time": 846430.0,
      "end_time": 851610.0,
      "text": "If the gradient exceeded a set value, for instance 15, it would just be reset and scaled"
    },
    {
      "index": 202,
      "start_time": 851610.0,
      "end_time": 854200.0,
      "text": "to that value."
    },
    {
      "index": 203,
      "start_time": 854200.0,
      "end_time": 857910.0,
      "text": "This was a common form of making RNNs much more stable."
    },
    {
      "index": 204,
      "start_time": 857910.0,
      "end_time": 862100.0,
      "text": "Interestingly though, at least on text analysis for sentiment, we don&#39;t seem to see this problem"
    },
    {
      "index": 205,
      "start_time": 862100.0,
      "end_time": 863440.0,
      "text": "with modern optimizers."
    },
    {
      "index": 206,
      "start_time": 863440.0,
      "end_time": 870380.0,
      "text": "It seems that the gradient decays pretty cleanly and becomes quite stable over the course of"
    },
    {
      "index": 207,
      "start_time": 870380.0,
      "end_time": 870870.0,
      "text": "learning."
    },
    {
      "index": 208,
      "start_time": 870870.0,
      "end_time": 874900.0,
      "text": "There&#39;s another way of making recurrent neural networks better and this is by using better"
    },
    {
      "index": 209,
      "start_time": 874900.0,
      "end_time": 875410.0,
      "text": "gating functions."
    },
    {
      "index": 210,
      "start_time": 875410.0,
      "end_time": 879960.0,
      "text": "There was an interesting paper this year at NIPS the basic idea of which was let&#39;s make"
    },
    {
      "index": 211,
      "start_time": 879960.0,
      "end_time": 886540.0,
      "text": "our gates steeper so they change more rapidly from being a value of zero to a value of one."
    },
    {
      "index": 212,
      "start_time": 886540.0,
      "end_time": 891660.0,
      "text": "What this means is a traditional sigmoid would change pretty smoothly between negative five"
    },
    {
      "index": 213,
      "start_time": 891660.0,
      "end_time": 892440.0,
      "text": "and five."
    },
    {
      "index": 214,
      "start_time": 892440.0,
      "end_time": 895760.0,
      "text": "But when you randomly initialize one of these numbers at the beginning of training typically"
    },
    {
      "index": 215,
      "start_time": 895760.0,
      "end_time": 900700.0,
      "text": "your values wouldn’t lie along the average 0.5, for instance."
    },
    {
      "index": 216,
      "start_time": 900700.0,
      "end_time": 902990.0,
      "text": "You wouldn’t see much dynamics here."
    },
    {
      "index": 217,
      "start_time": 902990.0,
      "end_time": 907480.0,
      "text": "If we make our gate steeper what that means is our gates begin to rapidly switch between"
    },
    {
      "index": 218,
      "start_time": 907480.0,
      "end_time": 911130.0,
      "text": "zero and one much more easily, particularly near the beginning of learning."
    },
    {
      "index": 219,
      "start_time": 911130.0,
      "end_time": 915920.0,
      "text": "What this seems to suggest is that models that have used these steeper gating units"
    },
    {
      "index": 220,
      "start_time": 915920.0,
      "end_time": 920290.0,
      "text": "tend to learn a bit faster because they begin to learn how to use these gates quicker."
    },
    {
      "index": 221,
      "start_time": 920290.0,
      "end_time": 922390.0,
      "text": "This is another quick easy technique to add."
    },
    {
      "index": 222,
      "start_time": 922390.0,
      "end_time": 926750.0,
      "text": "Again, the library we’ll be introducing later in this talk supports to help make learning"
    },
    {
      "index": 223,
      "start_time": 926750.0,
      "end_time": 928240.0,
      "text": "better in these models."
    },
    {
      "index": 224,
      "start_time": 928240.0,
      "end_time": 931840.0,
      "text": "Another technique is orthogonal initialization."
    },
    {
      "index": 225,
      "start_time": 931840.0,
      "end_time": 936530.0,
      "text": "Andrew Saxe last year did some great work on showing that initializing."
    },
    {
      "index": 226,
      "start_time": 936530.0,
      "end_time": 941000.0,
      "text": "When we begin training these models we don’t know the values of these parameters to use"
    },
    {
      "index": 227,
      "start_time": 941000.0,
      "end_time": 944960.0,
      "text": "in these dot products, for instance; the weight matrices effectively."
    },
    {
      "index": 228,
      "start_time": 944960.0,
      "end_time": 950030.0,
      "text": "What the research literature typically does is initialize [INAUDIBLE] for instance, random"
    },
    {
      "index": 229,
      "start_time": 950030.0,
      "end_time": 953450.0,
      "text": "Gaussian or random uniform noise."
    },
    {
      "index": 230,
      "start_time": 953450.0,
      "end_time": 958120.0,
      "text": "What this research showed is that using random orthogonal matrices worked much better."
    },
    {
      "index": 231,
      "start_time": 958120.0,
      "end_time": 961650.0,
      "text": "It&#39;s in line with some previous other work that has also noted various forms of similar"
    },
    {
      "index": 232,
      "start_time": 961650.0,
      "end_time": 964100.0,
      "text": "initializations worked well for RNNs."
    },
    {
      "index": 233,
      "start_time": 964100.0,
      "end_time": 967050.0,
      "text": "Now we want to understand how we train these models."
    },
    {
      "index": 234,
      "start_time": 967050.0,
      "end_time": 970640.0,
      "text": "There are a variety of techniques that can be used."
    },
    {
      "index": 235,
      "start_time": 970640.0,
      "end_time": 977270.0,
      "text": "This is a visualization of the training dynamics of various algorithms on toy datasets where"
    },
    {
      "index": 236,
      "start_time": 977270.0,
      "end_time": 981900.0,
      "text": "we’re trying to classify these red dots from these blue dots."
    },
    {
      "index": 237,
      "start_time": 981900.0,
      "end_time": 985930.0,
      "text": "We only have a linear model so all it can do is learn effectively a line separating"
    },
    {
      "index": 238,
      "start_time": 985930.0,
      "end_time": 987200.0,
      "text": "these two."
    },
    {
      "index": 239,
      "start_time": 987200.0,
      "end_time": 992760.0,
      "text": "It can&#39;t do it perfectly because there&#39;s always going to be values separating this."
    },
    {
      "index": 240,
      "start_time": 992760.0,
      "end_time": 999080.0,
      "text": "What we see is that the traditional most basic optimizer is stochastic gradient descent whereas"
    },
    {
      "index": 241,
      "start_time": 999080.0,
      "end_time": 1003030.0,
      "text": "there are these various other improvements and techniques."
    },
    {
      "index": 242,
      "start_time": 1003030.0,
      "end_time": 1009110.0,
      "text": "The main point of this example is to demonstrate to not use Sgd effectively."
    },
    {
      "index": 243,
      "start_time": 1009110.0,
      "end_time": 1014180.0,
      "text": "Sgd very early on in training can look quite similar but once the norm of your gradients"
    },
    {
      "index": 244,
      "start_time": 1014180.0,
      "end_time": 1020530.0,
      "text": "becomes slower due to later stages optimization you want some sort of dynamicism to your learning"
    },
    {
      "index": 245,
      "start_time": 1020530.0,
      "end_time": 1025059.0,
      "text": "algorithm whereas Sgd once it gets out of the very steep earlier areas of learning tends"
    },
    {
      "index": 246,
      "start_time": 1025060.0,
      "end_time": 1026340.0,
      "text": "to slow down."
    },
    {
      "index": 247,
      "start_time": 1026339.9999999999,
      "end_time": 1031459.9999999999,
      "text": "This is particularly a problem oftentimes in the space of text analysis because we have"
    },
    {
      "index": 248,
      "start_time": 1031460.0,
      "end_time": 1035160.0,
      "text": "very sparse updates on words, for instance."
    },
    {
      "index": 249,
      "start_time": 1035160.0000000001,
      "end_time": 1041530.0000000001,
      "text": "There are rare words that you only see once every thousand or 100,000 words and those"
    },
    {
      "index": 250,
      "start_time": 1041530.0,
      "end_time": 1045200.0,
      "text": "words are very difficult to learn in a traditional Sgd framework."
    },
    {
      "index": 251,
      "start_time": 1045200.0,
      "end_time": 1049361.0,
      "text": "Whereas these various techniques like momentum and [INAUDIBLE] accelerated gradient what"
    },
    {
      "index": 252,
      "start_time": 1049360.0,
      "end_time": 1054710.0,
      "text": "they do is effectively average together multiple updates and accumulate those averages."
    },
    {
      "index": 253,
      "start_time": 1054710.0,
      "end_time": 1060830.0,
      "text": "They’re a form of smoothing out this stochastic noise and accelerating directions of continuous"
    },
    {
      "index": 254,
      "start_time": 1060830.0,
      "end_time": 1062370.0,
      "text": "updates."
    },
    {
      "index": 255,
      "start_time": 1062370.0,
      "end_time": 1070129.0,
      "text": "There&#39;s another family of acceleration methods; the adafamily that effectively scale the learning"
    },
    {
      "index": 256,
      "start_time": 1070130.0,
      "end_time": 1078101.0,
      "text": "rate, the amount by which we update a parameter given a gradient by some dynamics, some heuristics"
    },
    {
      "index": 257,
      "start_time": 1078100.0,
      "end_time": 1080970.0,
      "text": "describing the local gradient."
    },
    {
      "index": 258,
      "start_time": 1080970.0,
      "end_time": 1085389.0,
      "text": "In the case of adagrad what we do is we accumulate the norm of the gradients update seen so far"
    },
    {
      "index": 259,
      "start_time": 1085390.0,
      "end_time": 1088170.0,
      "text": "with respect to a parameter and we scale our learning rate."
    },
    {
      "index": 260,
      "start_time": 1088170.0,
      "end_time": 1092701.0,
      "text": "It&#39;s a form of learning rate where we can see that early on it learns quite quickly"
    },
    {
      "index": 261,
      "start_time": 1092700.0,
      "end_time": 1098720.0,
      "text": "and later on it begins to slow down as it reaches in this case near [INAUDIBLE]."
    },
    {
      "index": 262,
      "start_time": 1098720.0,
      "end_time": 1103289.0,
      "text": "Adadelta an RMS prop do something a little bit like that but make it dynamic."
    },
    {
      "index": 263,
      "start_time": 1103290.0,
      "end_time": 1108810.0,
      "text": "It’s based on the local history instead of the global history of the gradients for"
    },
    {
      "index": 264,
      "start_time": 1108810.0,
      "end_time": 1109930.0,
      "text": "a parameter."
    },
    {
      "index": 265,
      "start_time": 1109930.0,
      "end_time": 1115481.0,
      "text": "There are a variety of optimizers and one recently introduced called ‘Adam’ combines"
    },
    {
      "index": 266,
      "start_time": 1115480.0,
      "end_time": 1120860.0,
      "text": "the early optimization speed that we saw in that earlier example of adagrad with the better"
    },
    {
      "index": 267,
      "start_time": 1120860.0,
      "end_time": 1125149.0,
      "text": "later convergence of various other methods like adadelta and RMS prop."
    },
    {
      "index": 268,
      "start_time": 1125150.0,
      "end_time": 1128841.0,
      "text": "This looks quite good for text analysis in RNN."
    },
    {
      "index": 269,
      "start_time": 1128840.0,
      "end_time": 1133320.0,
      "text": "We can see that Adam gets off to a very early learning start just like adagrad."
    },
    {
      "index": 270,
      "start_time": 1133320.0,
      "end_time": 1137470.0,
      "text": "These results…actually there&#39;s a slight bug in my code for this so take them with"
    },
    {
      "index": 271,
      "start_time": 1137470.0,
      "end_time": 1141120.0,
      "text": "a grain of salt but they still look good and it&#39;s a bug in the code so it might still be"
    },
    {
      "index": 272,
      "start_time": 1141120.0,
      "end_time": 1142460.0,
      "text": "okay."
    },
    {
      "index": 273,
      "start_time": 1142460.0,
      "end_time": 1147419.0,
      "text": "That might actually explain one of the reasons why we saw slightly worse generalization performance."
    },
    {
      "index": 274,
      "start_time": 1147420.0,
      "end_time": 1151091.0,
      "text": "It would train quite well but we would see its performance on held out data might not"
    },
    {
      "index": 275,
      "start_time": 1151090.0,
      "end_time": 1153350.0,
      "text": "have been as good for Adam because it learned so much more quickly."
    },
    {
      "index": 276,
      "start_time": 1153350.0,
      "end_time": 1157779.0,
      "text": "We’re still looking into reasons why this happens but in general, modern optimizers"
    },
    {
      "index": 277,
      "start_time": 1157780.0,
      "end_time": 1162300.0,
      "text": "are essential on these kinds of problems."
    },
    {
      "index": 278,
      "start_time": 1162300.0,
      "end_time": 1166621.0,
      "text": "This just gives you a background on all the various techniques for making RNNs more efficient"
    },
    {
      "index": 279,
      "start_time": 1166620.0,
      "end_time": 1169179.0,
      "text": "in training and it can add quite a lot."
    },
    {
      "index": 280,
      "start_time": 1169180.0,
      "end_time": 1173760.0,
      "text": "Early on in learning we can see that Adam and all these other techniques added together"
    },
    {
      "index": 281,
      "start_time": 1173760.0,
      "end_time": 1176751.0,
      "text": "so this would be a just a standard gating RNN."
    },
    {
      "index": 282,
      "start_time": 1176750.0,
      "end_time": 1181399.0,
      "text": "Again, if we had a simple RNN on here it would look pretty linear."
    },
    {
      "index": 283,
      "start_time": 1181400.0,
      "end_time": 1184800.0,
      "text": "If we add gradient clipping to make it more stable so we can use a slightly larger learning"
    },
    {
      "index": 284,
      "start_time": 1184800.0,
      "end_time": 1186961.0,
      "text": "rate it begins to learn faster."
    },
    {
      "index": 285,
      "start_time": 1186960.0,
      "end_time": 1191470.0,
      "text": "If we add orthogonal initialization we can see again that it began to learn faster and"
    },
    {
      "index": 286,
      "start_time": 1191470.0,
      "end_time": 1192360.0,
      "text": "learn better."
    },
    {
      "index": 287,
      "start_time": 1192360.0,
      "end_time": 1197269.0,
      "text": "Finally, if we had only Adam we see another huge gain over traditional Sgd."
    },
    {
      "index": 288,
      "start_time": 1197270.0,
      "end_time": 1197920.0,
      "text": "These add up."
    },
    {
      "index": 289,
      "start_time": 1197920.0,
      "end_time": 1203971.0,
      "text": "We can see that Adam and all these other techniques are able to reach lower effective minima and"
    },
    {
      "index": 290,
      "start_time": 1203970.0,
      "end_time": 1207210.0,
      "text": "are at least faster. Up to 10x faster."
    },
    {
      "index": 291,
      "start_time": 1207210.0,
      "end_time": 1211659.0,
      "text": "Admittedly these techniques add a little bit of computation time so it might only be for"
    },
    {
      "index": 292,
      "start_time": 1211660.0,
      "end_time": 1219270.0,
      "text": "instance 7.5x faster on a wall clock compared to efficiency per rate update."
    },
    {
      "index": 293,
      "start_time": 1219270.0,
      "end_time": 1224060.0,
      "text": "This is interesting because now RNNs can actually overfit quite a lot."
    },
    {
      "index": 294,
      "start_time": 1224060.0,
      "end_time": 1228520.0,
      "text": "As they continue to fit to training data for instance their test data might plateau."
    },
    {
      "index": 295,
      "start_time": 1228520.0,
      "end_time": 1232741.0,
      "text": "We continue to improve on the training dataset we’re given but this is called &#39;overfitting&#39;"
    },
    {
      "index": 296,
      "start_time": 1232740.0,
      "end_time": 1241289.0,
      "text": "where our RNN is effectively optimizing for the details of the training data that aren’t"
    },
    {
      "index": 297,
      "start_time": 1241290.0,
      "end_time": 1244121.0,
      "text": "true of new data."
    },
    {
      "index": 298,
      "start_time": 1244120.0,
      "end_time": 1247720.0,
      "text": "To combat this one of the techniques that is used is called ‘early stopping’ which"
    },
    {
      "index": 299,
      "start_time": 1247720.0,
      "end_time": 1253990.0,
      "text": "is each iteration of our dataset we will record the train and test validation training test"
    },
    {
      "index": 300,
      "start_time": 1253990.0,
      "end_time": 1259600.0,
      "text": "scores of these models and we will stop once we notice that our test validation performances"
    },
    {
      "index": 301,
      "start_time": 1259600.0,
      "end_time": 1261279.0,
      "text": "are improving."
    },
    {
      "index": 302,
      "start_time": 1261280.0,
      "end_time": 1264451.0,
      "text": "Oftentimes this is going to occur in your first or second iteration through the dataset"
    },
    {
      "index": 303,
      "start_time": 1264450.0,
      "end_time": 1266590.0,
      "text": "with all these various techniques together."
    },
    {
      "index": 304,
      "start_time": 1266590.0,
      "end_time": 1273090.0,
      "text": "That&#39;s good news because oftentimes models in this space can take ten, 50 or 100 iterations"
    },
    {
      "index": 305,
      "start_time": 1273090.0,
      "end_time": 1274659.0,
      "text": "for your training data to converge."
    },
    {
      "index": 306,
      "start_time": 1274660.0,
      "end_time": 1280241.0,
      "text": "It seems in the case of RNNs we often overfit after one or two epics through the data."
    },
    {
      "index": 307,
      "start_time": 1280240.0,
      "end_time": 1284559.0,
      "text": "To understand and get a better sense of how these models can do we’re going to compare"
    },
    {
      "index": 308,
      "start_time": 1284560.0,
      "end_time": 1287390.0,
      "text": "them to a much more standard technique in the literature."
    },
    {
      "index": 309,
      "start_time": 1287390.0,
      "end_time": 1291060.0,
      "text": "We’re going to use the Fantastic Machine Learning Library, SKLearn, and we’re going"
    },
    {
      "index": 310,
      "start_time": 1291060.0,
      "end_time": 1299461.0,
      "text": "to use a standard linear model approach, a traditional approach to text analysis."
    },
    {
      "index": 311,
      "start_time": 1299460.0,
      "end_time": 1305990.0,
      "text": "This would be using at TFIDFI vectorizer and a linear model such as logistic regression."
    },
    {
      "index": 312,
      "start_time": 1305990.0,
      "end_time": 1308570.0,
      "text": "This is by no means meant to be the best model."
    },
    {
      "index": 313,
      "start_time": 1308570.0,
      "end_time": 1314659.0,
      "text": "In many cases, naïve Bayes SVN is actually better than [INAUDIBLE] regression for classification"
    },
    {
      "index": 314,
      "start_time": 1314660.0,
      "end_time": 1320831.0,
      "text": "for instance, but this is just a very easily accessible, very easily comparable to technique"
    },
    {
      "index": 315,
      "start_time": 1320830.0,
      "end_time": 1323940.0,
      "text": "To be fair, we&#39;re going to use bigrams which is a way of getting a little bit of structure"
    },
    {
      "index": 316,
      "start_time": 1323940.0,
      "end_time": 1324830.0,
      "text": "into our data."
    },
    {
      "index": 317,
      "start_time": 1324830.0,
      "end_time": 1329840.0,
      "text": "Again, this way we could see ‘not good’ instead of just seeing the tokens for ‘not’"
    },
    {
      "index": 318,
      "start_time": 1329840.0,
      "end_time": 1330559.0,
      "text": "and ‘good’ occurring."
    },
    {
      "index": 319,
      "start_time": 1330560.0,
      "end_time": 1334390.0,
      "text": "We can get a little bit of structure which might be useful in sentiment analysis."
    },
    {
      "index": 320,
      "start_time": 1334390.0,
      "end_time": 1339691.0,
      "text": "We’re going to use grid search to evaluate potential [INAUDIBLE] for these linear models."
    },
    {
      "index": 321,
      "start_time": 1339690.0,
      "end_time": 1343080.0,
      "text": "We’re going to look at two which is minimum document frequency which is way of controlling"
    },
    {
      "index": 322,
      "start_time": 1343080.0,
      "end_time": 1346490.0,
      "text": "for the size of our input to our linear model."
    },
    {
      "index": 323,
      "start_time": 1346490.0,
      "end_time": 1352360.0,
      "text": "This would take tokens or words that appear less than, in less than and many documents"
    },
    {
      "index": 324,
      "start_time": 1352360.0,
      "end_time": 1353879.0,
      "text": "and would ignore them."
    },
    {
      "index": 325,
      "start_time": 1353880.0,
      "end_time": 1359081.0,
      "text": "If we see, for instance, the word ‘dinosaur’ and we&#39;ve only seen it once in our dataset"
    },
    {
      "index": 326,
      "start_time": 1359080.0,
      "end_time": 1361019.0,
      "text": "we&#39;re going to ignore it effectively."
    },
    {
      "index": 327,
      "start_time": 1361020.0,
      "end_time": 1366900.0,
      "text": "Also we&#39;re going to look at the regulization coefficient which is a way of preventing overfitting"
    },
    {
      "index": 328,
      "start_time": 1366900.0,
      "end_time": 1368140.0,
      "text": "for the new models."
    },
    {
      "index": 329,
      "start_time": 1368140.0,
      "end_time": 1372231.0,
      "text": "What we’re doing is grid search so we&#39;re looking at potential values for both of these."
    },
    {
      "index": 330,
      "start_time": 1372230.0,
      "end_time": 1378490.0,
      "text": "We&#39;re not just explaining a potential performance improvement based on poorly fitted parameters."
    },
    {
      "index": 331,
      "start_time": 1378490.0,
      "end_time": 1384830.0,
      "text": "Because these linear models tend to be faster we are able to more effectively search over"
    },
    {
      "index": 332,
      "start_time": 1384830.0,
      "end_time": 1385639.0,
      "text": "potential parameters."
    },
    {
      "index": 333,
      "start_time": 1385640.0,
      "end_time": 1390201.0,
      "text": "This is a fair way to get the linear model potential advantage because they&#39;re much faster"
    },
    {
      "index": 334,
      "start_time": 1390200.0,
      "end_time": 1393460.0,
      "text": "so we can much more quickly search through multiple values."
    },
    {
      "index": 335,
      "start_time": 1393460.0,
      "end_time": 1397929.0,
      "text": "Our second model we’re going to be looking at is one of these recurrent neural networks."
    },
    {
      "index": 336,
      "start_time": 1397930.0,
      "end_time": 1403871.0,
      "text": "Admittedly, this is our own personal research: take every result with a grain of salt."
    },
    {
      "index": 337,
      "start_time": 1403870.0,
      "end_time": 1407340.0,
      "text": "I&#39;m using whatever I’ve tried that worked."
    },
    {
      "index": 338,
      "start_time": 1407340.0,
      "end_time": 1412840.0,
      "text": "The general message though is that using a modern optimizer such as Adam, a gated recurrent"
    },
    {
      "index": 339,
      "start_time": 1412840.0,
      "end_time": 1418480.0,
      "text": "unit, steeper sigmoid gates and orthogonal initialization are good defaults."
    },
    {
      "index": 340,
      "start_time": 1418480.0,
      "end_time": 1425999.0,
      "text": "A medium-size model that can work quite well is a 256 dimensional embedding and a 512 dimensional"
    },
    {
      "index": 341,
      "start_time": 1426000.0,
      "end_time": 1428060.0,
      "text": "hidden representation."
    },
    {
      "index": 342,
      "start_time": 1428060.0,
      "end_time": 1434530.0,
      "text": "Then we put on whatever output we need: logistic regression for binary sentiment classification,"
    },
    {
      "index": 343,
      "start_time": 1434530.0,
      "end_time": 1438660.0,
      "text": "linear regression for predicting real values, etc."
    },
    {
      "index": 344,
      "start_time": 1438660.0,
      "end_time": 1445400.0,
      "text": "It&#39;s quite flexible because the RNN in its core is a way of taking these sequences of"
    },
    {
      "index": 345,
      "start_time": 1445400.0,
      "end_time": 1448510.0,
      "text": "values and converting them into a vector."
    },
    {
      "index": 346,
      "start_time": 1448510.0,
      "end_time": 1453571.0,
      "text": "Once we’ve got that vector we can put whatever traditional model we want on top of it so"
    },
    {
      "index": 347,
      "start_time": 1453570.0,
      "end_time": 1461259.0,
      "text": "long as it&#39;s differential and open to gradient based training."
    },
    {
      "index": 348,
      "start_time": 1461260.0,
      "end_time": 1464500.0,
      "text": "How does this work on datasets?"
    },
    {
      "index": 349,
      "start_time": 1464500.0,
      "end_time": 1469201.0,
      "text": "What we see quickly here is that our linear regression model does incredibly well for"
    },
    {
      "index": 350,
      "start_time": 1469200.0,
      "end_time": 1469990.0,
      "text": "smaller datasets."
    },
    {
      "index": 351,
      "start_time": 1469990.0,
      "end_time": 1475350.0,
      "text": "When we have for instance only 1,000 or 10,000 training examples we see that the linear model"
    },
    {
      "index": 352,
      "start_time": 1475350.0,
      "end_time": 1479730.0,
      "text": "outperforms the RNN by 50%, for instance."
    },
    {
      "index": 353,
      "start_time": 1479730.0,
      "end_time": 1484669.0,
      "text": "But what we notice that’s interesting as our datasets get bigger the RNN tends to scale"
    },
    {
      "index": 354,
      "start_time": 1484670.0,
      "end_time": 1487930.0,
      "text": "better until till later training into larger dataset sizes."
    },
    {
      "index": 355,
      "start_time": 1487930.0,
      "end_time": 1492721.0,
      "text": "Because the RNN is admittedly a much more complex model and operates on the sequences"
    },
    {
      "index": 356,
      "start_time": 1492720.0,
      "end_time": 1497809.0,
      "text": "themselves ideally with more training data it can learn a much better way to do the task"
    },
    {
      "index": 357,
      "start_time": 1497810.0,
      "end_time": 1498971.0,
      "text": "at hand."
    },
    {
      "index": 358,
      "start_time": 1498970.0,
      "end_time": 1503679.0,
      "text": "Whereas your linear model because it&#39;s operating on unstructured bag of words and is just a"
    },
    {
      "index": 359,
      "start_time": 1503680.0,
      "end_time": 1507971.0,
      "text": "linear model might eventually hit a wall where it&#39;s not able to do any better."
    },
    {
      "index": 360,
      "start_time": 1507970.0,
      "end_time": 1512899.0,
      "text": "You can imagine certain situations that you just aren&#39;t going to be able to classify the"
    },
    {
      "index": 361,
      "start_time": 1512900.0,
      "end_time": 1518591.0,
      "text": "sentiment, positiveness or negativeness of a text when it uses double negation, for instance."
    },
    {
      "index": 362,
      "start_time": 1518590.0,
      "end_time": 1521759.0,
      "text": "That’s one example with sentiment analysis."
    },
    {
      "index": 363,
      "start_time": 1521760.0,
      "end_time": 1525800.0,
      "text": "What’s also interesting is we see this replicated for instance for predicting the helpfulness"
    },
    {
      "index": 364,
      "start_time": 1525800.0,
      "end_time": 1527310.0,
      "text": "of a customer review."
    },
    {
      "index": 365,
      "start_time": 1527310.0,
      "end_time": 1530620.0,
      "text": "This is interesting because this is a much more qualitative thing."
    },
    {
      "index": 366,
      "start_time": 1530620.0,
      "end_time": 1536451.0,
      "text": "Sentiment is as well but how helpful a user’s review of a product is even more getting a"
    },
    {
      "index": 367,
      "start_time": 1536450.0,
      "end_time": 1538240.0,
      "text": "much more abstract concept."
    },
    {
      "index": 368,
      "start_time": 1538240.0,
      "end_time": 1544889.0,
      "text": "We see again that as before with small amounts of data the linear model, in this case reg"
    },
    {
      "index": 369,
      "start_time": 1544890.0,
      "end_time": 1549841.0,
      "text": "since we&#39;re predicting real values, does much better but it doesn&#39;t seem to scale and make"
    },
    {
      "index": 370,
      "start_time": 1549840.0,
      "end_time": 1554240.0,
      "text": "use of more data as effectively as an RNN."
    },
    {
      "index": 371,
      "start_time": 1554240.0,
      "end_time": 1555480.0,
      "text": "This is interesting."
    },
    {
      "index": 372,
      "start_time": 1555480.0,
      "end_time": 1560090.0,
      "text": "We can see that RNNs seem to have poor generalization properties with small amounts of data but"
    },
    {
      "index": 373,
      "start_time": 1560090.0,
      "end_time": 1562970.0,
      "text": "they seem to be doing better when we have large amounts of data."
    },
    {
      "index": 374,
      "start_time": 1562970.0,
      "end_time": 1568070.0,
      "text": "At one million labeled examples we can often be between zero and 30% better than the equivalent"
    },
    {
      "index": 375,
      "start_time": 1568070.0,
      "end_time": 1569309.0,
      "text": "linear model."
    },
    {
      "index": 376,
      "start_time": 1569310.0,
      "end_time": 1574770.0,
      "text": "Again these are just these examples with logistic regression and linear regression but that"
    },
    {
      "index": 377,
      "start_time": 1574770.0,
      "end_time": 1581430.0,
      "text": "crossover seems to be robust and somewhere between 100,000 and a million examples but"
    },
    {
      "index": 378,
      "start_time": 1581430.0,
      "end_time": 1584351.0,
      "text": "it is dependent on the dataset."
    },
    {
      "index": 379,
      "start_time": 1584350.0,
      "end_time": 1590090.0,
      "text": "There&#39;s only one unfortunate caveat to this approach which is it’s quite slow."
    },
    {
      "index": 380,
      "start_time": 1590090.0,
      "end_time": 1593909.0,
      "text": "For a million paragraph size text examples to converge that linear model takes about"
    },
    {
      "index": 381,
      "start_time": 1593910.0,
      "end_time": 1596201.0,
      "text": "30 minutes on a single CPU core."
    },
    {
      "index": 382,
      "start_time": 1596200.0,
      "end_time": 1602539.0,
      "text": "For an RNN if we use a high-powered graphics card such as the GTX 980 it takes about two"
    },
    {
      "index": 383,
      "start_time": 1602540.0,
      "end_time": 1602890.0,
      "text": "hours."
    },
    {
      "index": 384,
      "start_time": 1602890.0,
      "end_time": 1604321.0,
      "text": "That’s not too bad."
    },
    {
      "index": 385,
      "start_time": 1604320.0,
      "end_time": 1608960.0,
      "text": "Our RNN on a proper high-end graphics card is only about four times slower at a million"
    },
    {
      "index": 386,
      "start_time": 1608960.0,
      "end_time": 1611799.0,
      "text": "examples to converge than the linear model."
    },
    {
      "index": 387,
      "start_time": 1611800.0,
      "end_time": 1614500.0,
      "text": "Again this is on a basic CPU core."
    },
    {
      "index": 388,
      "start_time": 1614500.0,
      "end_time": 1619591.0,
      "text": "But if we train our RNN on just that CPU core it takes five days."
    },
    {
      "index": 389,
      "start_time": 1619590.0,
      "end_time": 1624470.0,
      "text": "This is unfortunate because this means our RNN is about 250 times slower than a CPU and"
    },
    {
      "index": 390,
      "start_time": 1624470.0,
      "end_time": 1626570.0,
      "text": "that&#39;s just not going to cut it."
    },
    {
      "index": 391,
      "start_time": 1626570.0,
      "end_time": 1630820.0,
      "text": "This effectively is why we use GPUs in this research."
    },
    {
      "index": 392,
      "start_time": 1630820.0,
      "end_time": 1633460.0,
      "text": "Here&#39;s the cool part of the presentation."
    },
    {
      "index": 393,
      "start_time": 1633460.0,
      "end_time": 1643110.0,
      "text": "Again, an RNN when it&#39;s being fed an input sequence takes in the sequence and effectively"
    },
    {
      "index": 394,
      "start_time": 1643110.0,
      "end_time": 1645239.0,
      "text": "learns a representation for each word."
    },
    {
      "index": 395,
      "start_time": 1645240.0,
      "end_time": 1652650.0,
      "text": "Each word gets replaced from its identifier some value like ‘the’ is token 100 and"
    },
    {
      "index": 396,
      "start_time": 1652650.0,
      "end_time": 1657030.0,
      "text": "gets replaced with a vector representation that is learned by our model."
    },
    {
      "index": 397,
      "start_time": 1657030.0,
      "end_time": 1661410.0,
      "text": "These visualizations we’ll be showing you are what happen when you look at what representations"
    },
    {
      "index": 398,
      "start_time": 1661410.0,
      "end_time": 1662701.0,
      "text": "are learned by those models."
    },
    {
      "index": 399,
      "start_time": 1662700.0,
      "end_time": 1668669.0,
      "text": "What we&#39;re going to do is use an algorithm called ‘TSNE’ to visualize these embeddings"
    },
    {
      "index": 400,
      "start_time": 1668670.0,
      "end_time": 1670961.0,
      "text": "that our RNN learns."
    },
    {
      "index": 401,
      "start_time": 1670960.0,
      "end_time": 1676009.0,
      "text": "What we&#39;ve done to make it a little clearer is this is the representations learned from"
    },
    {
      "index": 402,
      "start_time": 1676010.0,
      "end_time": 1678370.0,
      "text": "training on only binary sentiment analysis."
    },
    {
      "index": 403,
      "start_time": 1678370.0,
      "end_time": 1683740.0,
      "text": "We’re trying to predict whether a given customer review, for instance, likes a product"
    },
    {
      "index": 404,
      "start_time": 1683740.0,
      "end_time": 1685721.0,
      "text": "or doesn&#39;t like a product."
    },
    {
      "index": 405,
      "start_time": 1685720.0,
      "end_time": 1691090.0,
      "text": "What we&#39;ve done is we&#39;ve visualized these representations in two dimensions using TSNE"
    },
    {
      "index": 406,
      "start_time": 1691090.0,
      "end_time": 1698590.0,
      "text": "and we&#39;ve colored each word by the average sentiment of a review it appears in."
    },
    {
      "index": 407,
      "start_time": 1698590.0,
      "end_time": 1701179.0,
      "text": "What we see is a kind of axis."
    },
    {
      "index": 408,
      "start_time": 1701180.0,
      "end_time": 1706961.0,
      "text": "Again, it doesn&#39;t correspond to any actual axis aligned because it&#39;s TSNE."
    },
    {
      "index": 409,
      "start_time": 1706960.0,
      "end_time": 1713129.0,
      "text": "But we see this continuum between very negative words and very positive words."
    },
    {
      "index": 410,
      "start_time": 1713130.0,
      "end_time": 1714510.0,
      "text": "This isn&#39;t too surprising."
    },
    {
      "index": 411,
      "start_time": 1714510.0,
      "end_time": 1721071.0,
      "text": "A model trained on sentiment analysis learns to separate out negative and positive words."
    },
    {
      "index": 412,
      "start_time": 1721070.0,
      "end_time": 1722899.0,
      "text": "That&#39;s what you&#39;d expect to happen."
    },
    {
      "index": 413,
      "start_time": 1722900.0,
      "end_time": 1727471.0,
      "text": "We can take a little look at these very positive and very negative clusters and see that it&#39;s"
    },
    {
      "index": 414,
      "start_time": 1727470.0,
      "end_time": 1732309.0,
      "text": "grouped into very understandable words like ‘useless, waste, poorly, disappointed’"
    },
    {
      "index": 415,
      "start_time": 1732310.0,
      "end_time": 1733441.0,
      "text": "as negative."
    },
    {
      "index": 416,
      "start_time": 1733440.0,
      "end_time": 1739470.0,
      "text": "You can see some interesting stuff where again this visualization tries to group similar"
    },
    {
      "index": 417,
      "start_time": 1739470.0,
      "end_time": 1740480.0,
      "text": "things close together."
    },
    {
      "index": 418,
      "start_time": 1740480.0,
      "end_time": 1744919.0,
      "text": "We can see that it’s actually identified even though it is a very negative grouping,"
    },
    {
      "index": 419,
      "start_time": 1744920.0,
      "end_time": 1751060.0,
      "text": "it&#39;s also identified ‘returned, returning, returns, return’ all together as well."
    },
    {
      "index": 420,
      "start_time": 1751060.0,
      "end_time": 1755490.0,
      "text": "That’s interesting because it seems to know that ‘returned’ and return related words"
    },
    {
      "index": 421,
      "start_time": 1755490.0,
      "end_time": 1759091.0,
      "text": "are very negative unsurprisingly if you find them in a review."
    },
    {
      "index": 422,
      "start_time": 1759090.0,
      "end_time": 1763779.0,
      "text": "But it&#39;s also separated them out slightly from other more generic words."
    },
    {
      "index": 423,
      "start_time": 1763780.0,
      "end_time": 1769890.0,
      "text": "Then on the positive side we also see very unsurprising indicators of happy if sentiments."
    },
    {
      "index": 424,
      "start_time": 1769890.0,
      "end_time": 1772441.0,
      "text": "So ‘fantastic, wonderful, and pleased’."
    },
    {
      "index": 425,
      "start_time": 1772440.0,
      "end_time": 1776559.0,
      "text": "But what&#39;s even more interesting about this model is that we see other forms of grouping"
    },
    {
      "index": 426,
      "start_time": 1776560.0,
      "end_time": 1778650.0,
      "text": "and structure being learned."
    },
    {
      "index": 427,
      "start_time": 1778650.0,
      "end_time": 1783680.0,
      "text": "We see that it pulls out for instance, quantities of time; weeks, months, hours, minutes."
    },
    {
      "index": 428,
      "start_time": 1783680.0,
      "end_time": 1787701.0,
      "text": "We also see that it pulls out qualifiers like ‘really, absolutely, extremely, totally’."
    },
    {
      "index": 429,
      "start_time": 1787700.0,
      "end_time": 1791049.0,
      "text": "Again, qualifiers are interesting because they are by themselves neutral."
    },
    {
      "index": 430,
      "start_time": 1791050.0,
      "end_time": 1795680.0,
      "text": "They don&#39;t necessarily indicate positive or negative sentiment; instead they modify it."
    },
    {
      "index": 431,
      "start_time": 1795680.0,
      "end_time": 1799591.0,
      "text": "You can have ‘extremely good’ and ‘extremely bad’."
    },
    {
      "index": 432,
      "start_time": 1799590.0,
      "end_time": 1801340.0,
      "text": "You see that being pulled out together."
    },
    {
      "index": 433,
      "start_time": 1801340.0,
      "end_time": 1805730.0,
      "text": "You also see product nouns, for instance things that products could be, things that are products"
    },
    {
      "index": 434,
      "start_time": 1805730.0,
      "end_time": 1811399.0,
      "text": "like movies, books, stories, items, devices are also grouped together."
    },
    {
      "index": 435,
      "start_time": 1811400.0,
      "end_time": 1813920.0,
      "text": "Additionally, punctuation is grouped together."
    },
    {
      "index": 436,
      "start_time": 1813920.0,
      "end_time": 1820550.0,
      "text": "This is indicative potentially of our model learning to use these kinds of data which"
    },
    {
      "index": 437,
      "start_time": 1820550.0,
      "end_time": 1824530.0,
      "text": "again implies that our model may actually be learning to use some of the structure present"
    },
    {
      "index": 438,
      "start_time": 1824530.0,
      "end_time": 1826211.0,
      "text": "in the data."
    },
    {
      "index": 439,
      "start_time": 1826210.0,
      "end_time": 1829590.0,
      "text": "Punctuation by grouping it together and learning similar representations for it imply that"
    },
    {
      "index": 440,
      "start_time": 1829590.0,
      "end_time": 1831389.0,
      "text": "it&#39;s finding some use for it."
    },
    {
      "index": 441,
      "start_time": 1831390.0,
      "end_time": 1836040.0,
      "text": "We would expect again punctuation to be quite useful for segmenting out and separating out"
    },
    {
      "index": 442,
      "start_time": 1836040.0,
      "end_time": 1838630.0,
      "text": "meanings and notions."
    },
    {
      "index": 443,
      "start_time": 1838630.0,
      "end_time": 1840040.0,
      "text": "Quantities of time are interesting."
    },
    {
      "index": 444,
      "start_time": 1840040.0,
      "end_time": 1844581.0,
      "text": "They are slightly negatively associated which is understandable when you talk about, ‘this"
    },
    {
      "index": 445,
      "start_time": 1844580.0,
      "end_time": 1849809.0,
      "text": "product took months to show up’ or, ‘it worked for a total of an hour’."
    },
    {
      "index": 446,
      "start_time": 1849810.0,
      "end_time": 1855290.0,
      "text": "Again, grouping them all together implies some use of it and the same thing with qualifiers."
    },
    {
      "index": 447,
      "start_time": 1855290.0,
      "end_time": 1860790.0,
      "text": "We have no true evidence at least in this picture of these words being used but by learning"
    },
    {
      "index": 448,
      "start_time": 1860790.0,
      "end_time": 1864240.0,
      "text": "similar representations and by having them grouped together it implies it&#39;s finding a"
    },
    {
      "index": 449,
      "start_time": 1864240.0,
      "end_time": 1865101.0,
      "text": "use for them."
    },
    {
      "index": 450,
      "start_time": 1865100.0,
      "end_time": 1869509.0,
      "text": "We can extrapolate from there that it may in fact be learning to use these words in"
    },
    {
      "index": 451,
      "start_time": 1869510.0,
      "end_time": 1871731.0,
      "text": "natural ways for sentiment analysis."
    },
    {
      "index": 452,
      "start_time": 1871730.0,
      "end_time": 1877200.0,
      "text": "Again, this is learned purely from zero and one binary indicator variables."
    },
    {
      "index": 453,
      "start_time": 1877200.0,
      "end_time": 1884700.0,
      "text": "This is a bit like seeing a sequence of numbers, 1,000, 2,000, 3046, five and then realizing"
    },
    {
      "index": 454,
      "start_time": 1884700.0,
      "end_time": 1891269.0,
      "text": "that tokens five and one thousand are exclamation point and period."
    },
    {
      "index": 455,
      "start_time": 1891270.0,
      "end_time": 1897530.0,
      "text": "They&#39;re similar to tokens 2,000 and 7,000 which are comma and colon."
    },
    {
      "index": 456,
      "start_time": 1897530.0,
      "end_time": 1904160.0,
      "text": "This is a very strong result and very interesting to see this kind of similarity being learned"
    },
    {
      "index": 457,
      "start_time": 1904160.0,
      "end_time": 1905810.0,
      "text": "by our model."
    },
    {
      "index": 458,
      "start_time": 1905810.0,
      "end_time": 1909280.0,
      "text": "This is cool but how can we actually use these models?"
    },
    {
      "index": 459,
      "start_time": 1909280.0,
      "end_time": 1914000.0,
      "text": "We&#39;re also presenting today a basic library to allow developers to use these recurrent"
    },
    {
      "index": 460,
      "start_time": 1914000.0,
      "end_time": 1916120.0,
      "text": "neural networks for text analysis."
    },
    {
      "index": 461,
      "start_time": 1916120.0,
      "end_time": 1921420.0,
      "text": "It&#39;s called Passage and it’s a tiny library built on top of the great Theano machine learning"
    },
    {
      "index": 462,
      "start_time": 1921420.0,
      "end_time": 1925560.0,
      "text": "for framework [INAUDIBLE] math library."
    },
    {
      "index": 463,
      "start_time": 1925560.0,
      "end_time": 1929211.0,
      "text": "It&#39;s incredibly alpha; we&#39;re working on it but it has a variety of features."
    },
    {
      "index": 464,
      "start_time": 1929210.0,
      "end_time": 1933419.0,
      "text": "We&#39;re going to walk through now an example of how to use Passage."
    },
    {
      "index": 465,
      "start_time": 1933420.0,
      "end_time": 1934211.0,
      "text": "This is Passage."
    },
    {
      "index": 466,
      "start_time": 1934210.0,
      "end_time": 1939739.0,
      "text": "It’s clonable via GitHub and it has a variety of tools to make this useful."
    },
    {
      "index": 467,
      "start_time": 1939740.0,
      "end_time": 1942821.0,
      "text": "This is a little example we&#39;re going to walk through and explain real quickly on how we"
    },
    {
      "index": 468,
      "start_time": 1942820.0,
      "end_time": 1945840.0,
      "text": "can use Passage to do analysis of text."
    },
    {
      "index": 469,
      "start_time": 1945840.0,
      "end_time": 1948629.0,
      "text": "We need to import the components that are necessary."
    },
    {
      "index": 470,
      "start_time": 1948630.0,
      "end_time": 1954711.0,
      "text": "One of these is the tokenizer which is a way of taking strings of text and separating them"
    },
    {
      "index": 471,
      "start_time": 1954710.0,
      "end_time": 1959480.0,
      "text": "out into the individual tokens which would be words and punctuation, for instance."
    },
    {
      "index": 472,
      "start_time": 1959480.0,
      "end_time": 1961379.0,
      "text": "A tokenizer can just be instantiated."
    },
    {
      "index": 473,
      "start_time": 1961380.0,
      "end_time": 1965750.0,
      "text": "It has a variety of parameters but has sensible defaults."
    },
    {
      "index": 474,
      "start_time": 1965750.0,
      "end_time": 1968471.0,
      "text": "What we do is we emulate a SKLearn style interface."
    },
    {
      "index": 475,
      "start_time": 1968470.0,
      "end_time": 1973789.0,
      "text": "We can call fit transform on a body of training text which would be again a list of strings"
    },
    {
      "index": 476,
      "start_time": 1973790.0,
      "end_time": 1978461.0,
      "text": "for instance and that would return a list of these training tokens which can be used"
    },
    {
      "index": 477,
      "start_time": 1978460.0,
      "end_time": 1981509.0,
      "text": "natively by Passage to train RNN models."
    },
    {
      "index": 478,
      "start_time": 1981510.0,
      "end_time": 1985260.0,
      "text": "Additionally, we&#39;re going to import the various layers of a model."
    },
    {
      "index": 479,
      "start_time": 1985260.0,
      "end_time": 1991000.0,
      "text": "We have that embedding matrix we talked about, the gated recurrent unit, and a dense output"
    },
    {
      "index": 480,
      "start_time": 1991000.0,
      "end_time": 1992740.0,
      "text": "classifier."
    },
    {
      "index": 481,
      "start_time": 1992740.0,
      "end_time": 1998110.0,
      "text": "The way that we compose these into a training model is by stacking them together in a list."
    },
    {
      "index": 482,
      "start_time": 1998110.0,
      "end_time": 2005520.0,
      "text": "Our input is one of these embedding matrices and we&#39;re going to set it to have 128 dimensions."
    },
    {
      "index": 483,
      "start_time": 2005520.0,
      "end_time": 2009900.0,
      "text": "We need to know how many of these features to learn, how many of these tokens there are,"
    },
    {
      "index": 484,
      "start_time": 2009900.0,
      "end_time": 2014571.0,
      "text": "and we&#39;re going to just pull that out of how many our tokenizer decided we needed."
    },
    {
      "index": 485,
      "start_time": 2014570.0,
      "end_time": 2018009.0,
      "text": "Then we&#39;re going to use one of these gated recurrent layers where in this case setting"
    },
    {
      "index": 486,
      "start_time": 2018010.0,
      "end_time": 2019750.0,
      "text": "its size to 128."
    },
    {
      "index": 487,
      "start_time": 2019750.0,
      "end_time": 2024380.0,
      "text": "The sizes are sometimes smaller than you would use for actual models and you can see better"
    },
    {
      "index": 488,
      "start_time": 2024380.0,
      "end_time": 2028540.0,
      "text": "performance from larger models, for instance, but these are small enough to be run on a"
    },
    {
      "index": 489,
      "start_time": 2028540.0,
      "end_time": 2029870.0,
      "text": "CPU and not take forever."
    },
    {
      "index": 490,
      "start_time": 2029870.0,
      "end_time": 2032341.0,
      "text": "They’ll still take quite a while though."
    },
    {
      "index": 491,
      "start_time": 2032340.0,
      "end_time": 2037029.0,
      "text": "Finally, we have our dense output unit which would be if we were doing binary sentiment"
    },
    {
      "index": 492,
      "start_time": 2037030.0,
      "end_time": 2041441.0,
      "text": "classifications, detecting if it&#39;s negative or positive for a string of text, would be"
    },
    {
      "index": 493,
      "start_time": 2041440.0,
      "end_time": 2044019.0,
      "text": "one unit because we’d be predicting one value."
    },
    {
      "index": 494,
      "start_time": 2044020.0,
      "end_time": 2048911.0,
      "text": "We would use a sigmoid activation as a way of quickly separating out negative and positive"
    },
    {
      "index": 495,
      "start_time": 2048909.9999999998,
      "end_time": 2050369.9999999998,
      "text": "values."
    },
    {
      "index": 496,
      "start_time": 2050370.0,
      "end_time": 2056320.0,
      "text": "Then to make this model we instantiate it through the model class which is just importable"
    },
    {
      "index": 497,
      "start_time": 2056320.0000000002,
      "end_time": 2058490.0000000002,
      "text": "from Passage dot models RNN."
    },
    {
      "index": 498,
      "start_time": 2058489.9999999998,
      "end_time": 2062759.9999999998,
      "text": "We give it the layers we want to build our custom architecture out of and we tell it"
    },
    {
      "index": 499,
      "start_time": 2062760.0000000002,
      "end_time": 2065220.0000000002,
      "text": "what cost function we want to optimize."
    },
    {
      "index": 500,
      "start_time": 2065219.9999999998,
      "end_time": 2069649.9999999998,
      "text": "The cost function is the effective function that lets us train this model."
    },
    {
      "index": 501,
      "start_time": 2069650.0,
      "end_time": 2075010.0,
      "text": "It&#39;s just a way of telling the model how good was this, how good did you do on this example,"
    },
    {
      "index": 502,
      "start_time": 2075010.0000000002,
      "end_time": 2075290.0000000002,
      "text": "effectively."
    },
    {
      "index": 503,
      "start_time": 2075290.0,
      "end_time": 2079800.0,
      "text": "For binary classification we use binary [INAUDIBLE] in this example."
    },
    {
      "index": 504,
      "start_time": 2079800.0000000002,
      "end_time": 2084750.0000000002,
      "text": "To train this model we just call a fit interface which takes in training tokens which are made"
    },
    {
      "index": 505,
      "start_time": 2084750.0,
      "end_time": 2090630.0,
      "text": "from training text and also takes in the training labels we want to predict given those training"
    },
    {
      "index": 506,
      "start_time": 2090630.0,
      "end_time": 2091820.0,
      "text": "texts."
    },
    {
      "index": 507,
      "start_time": 2091820.0000000002,
      "end_time": 2095790.0000000002,
      "text": "Then once that model has been trained…It should be noted this only transfers one iteration"
    },
    {
      "index": 508,
      "start_time": 2095790.0,
      "end_time": 2097260.0,
      "text": "through your dataset."
    },
    {
      "index": 509,
      "start_time": 2097260.0,
      "end_time": 2101240.0,
      "text": "As mentioned earlier, you may want to train for multiple iterations if for instance your"
    },
    {
      "index": 510,
      "start_time": 2101240.0,
      "end_time": 2105130.0,
      "text": "model hasn’t converged and you may want to measure your performance on hold out data"
    },
    {
      "index": 511,
      "start_time": 2105130.0,
      "end_time": 2109120.0,
      "text": "to know when to stop training a model if it begins to overfit."
    },
    {
      "index": 512,
      "start_time": 2109120.0,
      "end_time": 2113530.0,
      "text": "Right now we&#39;ve left that part to you but we will be extending this to have interfaces"
    },
    {
      "index": 513,
      "start_time": 2113530.0,
      "end_time": 2115490.0,
      "text": "to automatically do this."
    },
    {
      "index": 514,
      "start_time": 2115490.0,
      "end_time": 2119510.0,
      "text": "Finally, if you want to have your model then predict on new data you can just call model"
    },
    {
      "index": 515,
      "start_time": 2119510.0,
      "end_time": 2125770.0,
      "text": "dot predict on tokenizer dot transform or test text and this will return how the model"
    },
    {
      "index": 516,
      "start_time": 2125770.0,
      "end_time": 2127730.0,
      "text": "predicts new data."
    },
    {
      "index": 517,
      "start_time": 2127730.0,
      "end_time": 2130170.0,
      "text": "That&#39;s an example of how to use Passage."
    },
    {
      "index": 518,
      "start_time": 2130170.0,
      "end_time": 2135010.0,
      "text": "To summarize, RNNs are now a potentially competitive tool in certain situations for text analysis."
    },
    {
      "index": 519,
      "start_time": 2135010.0,
      "end_time": 2139390.0,
      "text": "Admittedly there are a lot of disclaimers there but there seems to be a general trend"
    },
    {
      "index": 520,
      "start_time": 2139390.0,
      "end_time": 2143090.0,
      "text": "that seems to be emerging which is if you have a large, for instance, million-plus example"
    },
    {
      "index": 521,
      "start_time": 2143090.0,
      "end_time": 2146430.0,
      "text": "dataset and you have a GPU they can look quite good."
    },
    {
      "index": 522,
      "start_time": 2146430.0,
      "end_time": 2151600.0,
      "text": "They potentially can outperform linear models and might not take all that much longer."
    },
    {
      "index": 523,
      "start_time": 2151600.0,
      "end_time": 2156710.0,
      "text": "But if you have a smaller dataset and don&#39;t have a GPU it can be very difficult to justify"
    },
    {
      "index": 524,
      "start_time": 2156710.0,
      "end_time": 2160160.0,
      "text": "despite how cool these models might seem compared to linear models."
    },
    {
      "index": 525,
      "start_time": 2160160.0,
      "end_time": 2163980.0,
      "text": "They are a lot slower, they have a lot of complexity, a lot of different parts and a"
    },
    {
      "index": 526,
      "start_time": 2163980.0,
      "end_time": 2167790.0,
      "text": "lot of different architectures you can change and they seem to have poor generalization"
    },
    {
      "index": 527,
      "start_time": 2167790.0,
      "end_time": 2170510.0,
      "text": "results with small datasets."
    },
    {
      "index": 528,
      "start_time": 2170510.0,
      "end_time": 2171770.0,
      "text": "Thanks for listening."
    },
    {
      "index": 529,
      "start_time": 2171770.0,
      "end_time": 2175660.0,
      "text": "If you have any questions you can let me know at Alec at Indico dot I-O."
    },
    {
      "index": 530,
      "start_time": 2175660.0,
      "end_time": 2181790.0,
      "text": "Also if you&#39;d like to see a more general introduction to machine learning and deep learning in Python"
    },
    {
      "index": 531,
      "start_time": 2181790.0,
      "end_time": 2185720.0,
      "text": "I have another video that you can check out in the upper right, introducing that as a"
    },
    {
      "index": 532,
      "start_time": 2185720.0,
      "end_time": 2192790.0,
      "text": "Python developer, how to use the awesome Theano library to implement these algorithms yourself."
    },
    {
      "index": 533,
      "start_time": 2192790.0,
      "end_time": 2196390.0,
      "text": "Additionally if you&#39;d like to check out and learn more about Indico feel free to visit"
    },
    {
      "index": 534,
      "start_time": 2196390.0,
      "end_time": 2201740.0,
      "text": "our website at Indico dot I-O where we have various tools like Passage available for developers"
    },
    {
      "index": 535,
      "start_time": 2201740.0,
      "end_time": 2203100.0,
      "text": "to use for machine learning."
    },
    {
      "index": 536,
      "start_time": 2203100.0,
      "end_time": 2213100.0,
      "text": "Thanks."
    }
  ]
}