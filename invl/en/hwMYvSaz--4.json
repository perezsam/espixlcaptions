{
  "video_id": "hwMYvSaz--4",
  "title": "FLOPS",
  "es": 0,
  "json": [
    {
      "index": 1,
      "start_time": 0.0,
      "end_time": 5220.0,
      "text": "In computing, FLOPS is a measure of computer performance, useful in fields of scientific"
    },
    {
      "index": 2,
      "start_time": 5220.0,
      "end_time": 11259.0,
      "text": "calculations that make heavy use of floating-point calculations. For such cases it is a more"
    },
    {
      "index": 3,
      "start_time": 11259.0,
      "end_time": 14889.0,
      "text": "accurate measure than the generic instructions per second."
    },
    {
      "index": 4,
      "start_time": 14889.0,
      "end_time": 20880.0,
      "text": "Since the final S stands for &quot;second&quot;, conservative speakers consider &quot;FLOPS&quot; as both the singular"
    },
    {
      "index": 5,
      "start_time": 20880.0,
      "end_time": 27300.0,
      "text": "and plural of the term, although the singular &quot;FLOP&quot; is frequently encountered. Alternatively,"
    },
    {
      "index": 6,
      "start_time": 27300.0,
      "end_time": 32189.0,
      "text": "the singular FLOP is used as an abbreviation for &quot;FLoating-point OPeration&quot;, and a flop"
    },
    {
      "index": 7,
      "start_time": 32189.0,
      "end_time": 38739.0,
      "text": "count is a count of these operations. In this context, &quot;flops&quot; is simply the plural rather"
    },
    {
      "index": 8,
      "start_time": 38739.0,
      "end_time": 44170.0,
      "text": "than a rate, which would then be &quot;flop/s&quot;. The expression 1 flops is actually interpreted"
    },
    {
      "index": 9,
      "start_time": 44170.0,
      "end_time": 45969.0,
      "text": "as ."
    },
    {
      "index": 10,
      "start_time": 45969.0,
      "end_time": 49940.0,
      "text": "Computing One can calculate FLOPS using this equation:"
    },
    {
      "index": 11,
      "start_time": 49940.0,
      "end_time": 56050.0,
      "text": "Most microprocessors today can do 4 FLOPs per clock cycle. Therefore, a single-core"
    },
    {
      "index": 12,
      "start_time": 56050.0,
      "end_time": 62969.0,
      "text": "2.5 GHz processor has a theoretical performance of 10 billion FLOPS = 10 GFLOPS."
    },
    {
      "index": 13,
      "start_time": 62969.0,
      "end_time": 70170.0,
      "text": "Note: In this context, sockets is referring to chip sockets on a motherboard, in other"
    },
    {
      "index": 14,
      "start_time": 70170.0,
      "end_time": 75439.0,
      "text": "words, how many computer chips are in use, with each chip having one or more cores on"
    },
    {
      "index": 15,
      "start_time": 75439.0,
      "end_time": 80780.0,
      "text": "it. This equation only applies to one very specific hardware architecture and it ignores"
    },
    {
      "index": 16,
      "start_time": 80780.0,
      "end_time": 87100.0,
      "text": "limits imposed by memory bandwidth and other constraints. In general, GigaFLOPS are not"
    },
    {
      "index": 17,
      "start_time": 87100.0,
      "end_time": 92979.0,
      "text": "determined by theoretical calculations such as this one; instead, they are measured by"
    },
    {
      "index": 18,
      "start_time": 92979.0,
      "end_time": 98560.0,
      "text": "benchmarks of actual performance/throughput. Because this equation ignores all sources"
    },
    {
      "index": 19,
      "start_time": 98560.0,
      "end_time": 103310.0,
      "text": "of overhead, in the real world, one will never get actual performance that is anywhere near"
    },
    {
      "index": 20,
      "start_time": 103310.0,
      "end_time": 107060.0,
      "text": "to what this equation predicts. Records"
    },
    {
      "index": 21,
      "start_time": 107060.0,
      "end_time": 112509.0,
      "text": "Single computer records In late 1996, Intel&#39;s ASCI Red was the world&#39;s"
    },
    {
      "index": 22,
      "start_time": 112509.0,
      "end_time": 120250.0,
      "text": "first computer to achieve one TFLOPS and beyond. Sandia director Bill Camp said that ASCI Red"
    },
    {
      "index": 23,
      "start_time": 120250.0,
      "end_time": 128030.0,
      "text": "had the best reliability of any supercomputer ever built, and “was supercomputing’s"
    },
    {
      "index": 24,
      "start_time": 128030.0,
      "end_time": 135569.0,
      "text": "high-water mark in longevity, price, and performance.” NEC&#39;s SX-9 supercomputer was the world&#39;s first"
    },
    {
      "index": 25,
      "start_time": 135569.0,
      "end_time": 139890.0,
      "text": "vector processor to exceed 100 gigaFLOPS per single core."
    },
    {
      "index": 26,
      "start_time": 139890.0,
      "end_time": 146590.0,
      "text": "For comparison, a handheld calculator performs relatively few FLOPS. A computer response"
    },
    {
      "index": 27,
      "start_time": 146590.0,
      "end_time": 152620.0,
      "text": "time below 0.1 second in a calculation context is usually perceived as instantaneous by a"
    },
    {
      "index": 28,
      "start_time": 152620.0,
      "end_time": 159230.0,
      "text": "human operator, so a simple calculator needs only about 10 FLOPS to be considered functional."
    },
    {
      "index": 29,
      "start_time": 159230.0,
      "end_time": 166099.0,
      "text": "In June 2006, a new computer was announced by Japanese research institute RIKEN, the"
    },
    {
      "index": 30,
      "start_time": 166099.0,
      "end_time": 172080.0,
      "text": "MDGRAPE-3. The computer&#39;s performance tops out at one petaFLOPS, almost two times faster"
    },
    {
      "index": 31,
      "start_time": 172080.0,
      "end_time": 177080.0,
      "text": "than the Blue Gene/L, but MDGRAPE-3 is not a general purpose computer, which is why it"
    },
    {
      "index": 32,
      "start_time": 177080.0,
      "end_time": 183319.0,
      "text": "does not appear in the Top500.org list. It has special-purpose pipelines for simulating"
    },
    {
      "index": 33,
      "start_time": 183319.0,
      "end_time": 189550.0,
      "text": "molecular dynamics. By 2007, Intel Corporation unveiled the experimental"
    },
    {
      "index": 34,
      "start_time": 189550.0,
      "end_time": 198150.0,
      "text": "multi-core POLARIS chip, which achieves 1 TFLOPS at 3.13 GHz. The 80-core chip can raise this"
    },
    {
      "index": 35,
      "start_time": 198150.0,
      "end_time": 205230.0,
      "text": "result to 2 TFLOPS at 6.26 GHz, although the thermal dissipation at this frequency"
    },
    {
      "index": 36,
      "start_time": 205230.0,
      "end_time": 212159.0,
      "text": "exceeds 190 watts. On June 26, 2007, IBM announced the second"
    },
    {
      "index": 37,
      "start_time": 212159.0,
      "end_time": 217569.0,
      "text": "generation of its top supercomputer, dubbed Blue Gene/P and designed to continuously operate"
    },
    {
      "index": 38,
      "start_time": 217569.0,
      "end_time": 224030.0,
      "text": "at speeds exceeding one petaFLOPS. When configured to do so, it can reach speeds in excess of"
    },
    {
      "index": 39,
      "start_time": 224030.0,
      "end_time": 230220.0,
      "text": "three petaFLOPS. In June 2007, Top500.org reported the fastest"
    },
    {
      "index": 40,
      "start_time": 230220.0,
      "end_time": 238459.0,
      "text": "computer in the world to be the IBM Blue Gene/L supercomputer, measuring a peak of 596 teraFLOPS."
    },
    {
      "index": 41,
      "start_time": 238459.0,
      "end_time": 248739.0,
      "text": "The Cray XT4 hit second place with 101.7 teraFLOPS. On October 25, 2007, NEC Corporation of Japan"
    },
    {
      "index": 42,
      "start_time": 248739.0,
      "end_time": 254260.0,
      "text": "issued a press release announcing its SX series model SX-9, claiming it to be the world&#39;s"
    },
    {
      "index": 43,
      "start_time": 254260.0,
      "end_time": 261389.0,
      "text": "fastest vector supercomputer. The SX-9 features the first CPU capable of a peak vector performance"
    },
    {
      "index": 44,
      "start_time": 261389.0,
      "end_time": 269350.0,
      "text": "of 102.4 gigaFLOPS per single core. On February 4, 2008, the NSF and the University"
    },
    {
      "index": 45,
      "start_time": 269350.0,
      "end_time": 276150.0,
      "text": "of Texas at Austin opened full scale research runs on an AMD, Sun supercomputer named Ranger,"
    },
    {
      "index": 46,
      "start_time": 276150.0,
      "end_time": 281080.0,
      "text": "the most powerful supercomputing system in the world for open science research, which"
    },
    {
      "index": 47,
      "start_time": 281080.0,
      "end_time": 288930.0,
      "text": "operates at sustained speed of .5 petaFLOPS. On May 25, 2008, an American supercomputer"
    },
    {
      "index": 48,
      "start_time": 288930.0,
      "end_time": 294490.0,
      "text": "built by IBM, named &#39;Roadrunner&#39;, reached the computing milestone of one petaflops by"
    },
    {
      "index": 49,
      "start_time": 294490.0,
      "end_time": 302979.0,
      "text": "processing more than 1.026 quadrillion calculations per second. It headed the June 2008 and November"
    },
    {
      "index": 50,
      "start_time": 302979.0,
      "end_time": 311340.0,
      "text": "2008 TOP500 list of the most powerful supercomputers. The computer is located at Los Alamos National"
    },
    {
      "index": 51,
      "start_time": 311340.0,
      "end_time": 316199.0,
      "text": "Laboratory in New Mexico, and the computer&#39;s name refers to the New Mexico state bird,"
    },
    {
      "index": 52,
      "start_time": 316199.0,
      "end_time": 323310.0,
      "text": "the Greater Roadrunner. In June 2008, AMD released ATI Radeon HD4800"
    },
    {
      "index": 53,
      "start_time": 323310.0,
      "end_time": 329490.0,
      "text": "series, which are reported to be the first GPUs to achieve one teraFLOPS scale. On August"
    },
    {
      "index": 54,
      "start_time": 329490.0,
      "end_time": 339729.0,
      "text": "12, 2008 AMD released the ATI Radeon HD 4870X2 graphics card with two Radeon R770 GPUs totaling"
    },
    {
      "index": 55,
      "start_time": 339729.0,
      "end_time": 344740.0,
      "text": "2.4 teraFLOPS. In November 2008, an upgrade to the Cray XT"
    },
    {
      "index": 56,
      "start_time": 344740.0,
      "end_time": 351810.0,
      "text": "Jaguar supercomputer at the Department of Energy’s Oak Ridge National Laboratory raised"
    },
    {
      "index": 57,
      "start_time": 351810.0,
      "end_time": 358979.0,
      "text": "the system&#39;s computing power to a peak 1.64 “petaflops,” or a quadrillion mathematical"
    },
    {
      "index": 58,
      "start_time": 358979.0,
      "end_time": 366530.0,
      "text": "calculations per second, making Jaguar the world’s first petaflops system dedicated"
    },
    {
      "index": 59,
      "start_time": 366530.0,
      "end_time": 373690.0,
      "text": "to open research. In early 2009 the supercomputer was named after a mythical creature, Kraken."
    },
    {
      "index": 60,
      "start_time": 373690.0,
      "end_time": 380350.0,
      "text": "Kraken was declared the world&#39;s fastest university-managed supercomputer and sixth fastest overall in"
    },
    {
      "index": 61,
      "start_time": 380350.0,
      "end_time": 387520.0,
      "text": "the 2009 TOP500 list, which is the global standard for ranking supercomputers. In 2010"
    },
    {
      "index": 62,
      "start_time": 387520.0,
      "end_time": 392380.0,
      "text": "Kraken was upgraded and can operate faster and is more powerful."
    },
    {
      "index": 63,
      "start_time": 392380.0,
      "end_time": 399280.0,
      "text": "In 2009, the Cray Jaguar performed at 1.75 petaFLOPS, beating the IBM Roadrunner for"
    },
    {
      "index": 64,
      "start_time": 399280.0,
      "end_time": 406360.0,
      "text": "the number one spot on the TOP500 list. In October 2010, China unveiled the Tianhe-I,"
    },
    {
      "index": 65,
      "start_time": 406360.0,
      "end_time": 411500.0,
      "text": "a supercomputer that operates at a peak computing rate of 2.5 petaflops."
    },
    {
      "index": 66,
      "start_time": 411500.0,
      "end_time": 419319.0,
      "text": "As of 2010, the fastest six-core PC processor reaches 109 gigaFLOPS in double precision"
    },
    {
      "index": 67,
      "start_time": 419319.0,
      "end_time": 429830.0,
      "text": "calculations. GPUs are considerably more powerful. For example, Nvidia Tesla C2050 GPU computing"
    },
    {
      "index": 68,
      "start_time": 429830.0,
      "end_time": 435710.0,
      "text": "processors perform around 515 gigaFLOPS in double precision calculations, and the AMD"
    },
    {
      "index": 69,
      "start_time": 435710.0,
      "end_time": 444479.0,
      "text": "FireStream 9270 peaks at 240 gigaFLOPS. In single precision performance, Nvidia Tesla"
    },
    {
      "index": 70,
      "start_time": 444479.0,
      "end_time": 451520.0,
      "text": "C2050 computing processors perform around 1.03 teraFLOPS and the AMD FireStream 9270"
    },
    {
      "index": 71,
      "start_time": 451520.0,
      "end_time": 460039.0,
      "text": "cards peak at 1.2 teraFLOPS. Both Nvidia and AMD&#39;s consumer gaming GPUs may reach higher"
    },
    {
      "index": 72,
      "start_time": 460039.0,
      "end_time": 470250.0,
      "text": "FLOPS. For example, AMD’s HemlockXT 5970 reaches 928 gigaFLOPS in double precision"
    },
    {
      "index": 73,
      "start_time": 470250.0,
      "end_time": 477120.0,
      "text": "calculations with two GPUs on board and the Nvidia GTX 480 reaches 672 gigaFLOPS with"
    },
    {
      "index": 74,
      "start_time": 477120.0,
      "end_time": 483870.0,
      "text": "one GPU on board. On December 2, 2010, the US Air Force unveiled"
    },
    {
      "index": 75,
      "start_time": 483870.0,
      "end_time": 492580.0,
      "text": "a defense supercomputer made up of 1,760 PlayStation 3 consoles that can run 500 trillion floating-point"
    },
    {
      "index": 76,
      "start_time": 492580.0,
      "end_time": 497500.0,
      "text": "operations per second. In November 2011, it was announced that Japan"
    },
    {
      "index": 77,
      "start_time": 497500.0,
      "end_time": 504139.0,
      "text": "had achieved 10.51 petaflops with its K computer. It is still under development and software"
    },
    {
      "index": 78,
      "start_time": 504139.0,
      "end_time": 512959.0,
      "text": "performance tuning is currently underway. It has 88,128 SPARC64 VIIIfx processors in"
    },
    {
      "index": 79,
      "start_time": 512958.99999999994,
      "end_time": 520579.99999999994,
      "text": "864 racks, with theoretical performance of 11.28 petaflops. It is named after the Japanese"
    },
    {
      "index": 80,
      "start_time": 520580.00000000006,
      "end_time": 527670.0,
      "text": "word &quot;kei&quot;, which stands for 10 quadrillion, corresponding to the target speed of 10 petaFLOPS."
    },
    {
      "index": 81,
      "start_time": 527670.0,
      "end_time": 534750.0,
      "text": "On November 15, 2011, Intel demonstrated a single x86-based processor, code-named &quot;Knights"
    },
    {
      "index": 82,
      "start_time": 534750.0,
      "end_time": 541760.0,
      "text": "Corner&quot;, sustaining more than a TeraFlop on a wide range of DGEMM operations. Intel emphasized"
    },
    {
      "index": 83,
      "start_time": 541760.0,
      "end_time": 546790.0,
      "text": "during the demonstration that this was a sustained TeraFlop, and that it was the first general"
    },
    {
      "index": 84,
      "start_time": 546790.0,
      "end_time": 554720.0,
      "text": "purpose processor to ever cross a TeraFlop. On June 18, 2012, IBM&#39;s Sequoia supercomputer"
    },
    {
      "index": 85,
      "start_time": 554720.0,
      "end_time": 560500.0,
      "text": "system, based at the U.S. Lawrence Livermore National Laboratory, reached 16 petaFLOPS,"
    },
    {
      "index": 86,
      "start_time": 560500.0,
      "end_time": 566490.0,
      "text": "setting the world record and claiming first place in the latest TOP500 list."
    },
    {
      "index": 87,
      "start_time": 566490.0,
      "end_time": 573950.0,
      "text": "On November 12, 2012, the TOP500 list certified Titan as the world&#39;s fastest supercomputer"
    },
    {
      "index": 88,
      "start_time": 573950.0,
      "end_time": 581990.0,
      "text": "per the LINPACK benchmark, at 17.59 petaFLOPS. It was developed by Cray Inc. at the Oak Ridge"
    },
    {
      "index": 89,
      "start_time": 581990.0,
      "end_time": 589560.0,
      "text": "National Laboratory and combines AMD Opteron processors with “Kepler” NVIDIA Tesla"
    },
    {
      "index": 90,
      "start_time": 589560.0,
      "end_time": 596040.0,
      "text": "graphic processing unit technologies. On June 10, 2013, China&#39;s Tianhe-2 was ranked"
    },
    {
      "index": 91,
      "start_time": 596040.0,
      "end_time": 601079.0,
      "text": "the world&#39;s fastest with a record of 33.86 petaflops."
    },
    {
      "index": 92,
      "start_time": 601079.0,
      "end_time": 613070.0,
      "text": "On April 8, 2014, AMD launched R9 295X2, a dual R9 290X in a single PCB, with 11.6 TFlops."
    },
    {
      "index": 93,
      "start_time": 613070.0,
      "end_time": 617440.0,
      "text": "Distributed computing records Distributed computing uses the Internet to"
    },
    {
      "index": 94,
      "start_time": 617440.0,
      "end_time": 623720.0,
      "text": "link personal computers to achieve more FLOPS: Folding@home is sustaining over 20.7 native"
    },
    {
      "index": 95,
      "start_time": 623720.0,
      "end_time": 632070.0,
      "text": "petaFLOPS as of June 2014 or 43.1 x86 petaFLOPS. It is the first computing project of any kind"
    },
    {
      "index": 96,
      "start_time": 632070.0,
      "end_time": 640160.0,
      "text": "to cross the 1, 2, 3, 4, and 5 native petaFLOPS milestone. This level of performance is primarily"
    },
    {
      "index": 97,
      "start_time": 640160.0,
      "end_time": 645990.0,
      "text": "enabled by the cumulative effort of a vast array of powerful GPU and CPU units."
    },
    {
      "index": 98,
      "start_time": 645990.0,
      "end_time": 652779.0,
      "text": "As of July 2014, The entire BOINC network averages about 5.6 petaFLOPS."
    },
    {
      "index": 99,
      "start_time": 652779.0,
      "end_time": 664000.0,
      "text": "As of July 2014, SETI@Home, employing the BOINC software platform, averages 681 teraFLOPS."
    },
    {
      "index": 100,
      "start_time": 664000.0,
      "end_time": 670980.0,
      "text": "As of July 2014, Einstein@Home, a project using the BOINC network , is crunching at"
    },
    {
      "index": 101,
      "start_time": 670980.0,
      "end_time": 678470.0,
      "text": "492 teraFLOPS. As of July 2014, MilkyWay@Home, using the"
    },
    {
      "index": 102,
      "start_time": 678470.0,
      "end_time": 687380.0,
      "text": "BOINC infrastructure, computes at 471 teraFLOPS. As of July 2014, GIMPS, is searching for Mersenne"
    },
    {
      "index": 103,
      "start_time": 687380.0,
      "end_time": 692640.0,
      "text": "primes and sustaining 173 teraFLOPS. Future developments"
    },
    {
      "index": 104,
      "start_time": 692640.0,
      "end_time": 699390.0,
      "text": "In 2008, James Bamford&#39;s book The Shadow Factory reported that NSA told the Pentagon it would"
    },
    {
      "index": 105,
      "start_time": 699390.0,
      "end_time": 706290.0,
      "text": "need an exaflop computer by 2018. Given the current speed of progress, supercomputers"
    },
    {
      "index": 106,
      "start_time": 706290.0,
      "end_time": 714029.0,
      "text": "are projected to reach 1 exaFLOPS in 2019. Cray, Inc. announced in December 2009 a plan"
    },
    {
      "index": 107,
      "start_time": 714029.0,
      "end_time": 721600.0,
      "text": "to build a 1 EFLOPS supercomputer before 2020. Erik P. DeBenedictis of Sandia National Laboratories"
    },
    {
      "index": 108,
      "start_time": 721600.0,
      "end_time": 726120.0,
      "text": "theorizes that a zettaFLOPS computer is required to accomplish full weather modeling of two"
    },
    {
      "index": 109,
      "start_time": 726120.0,
      "end_time": 731690.0,
      "text": "week time span. Such systems might be built around 2030."
    },
    {
      "index": 110,
      "start_time": 731690.0,
      "end_time": 736540.0,
      "text": "In India, ISRO and Indian Institute of Science have stated that they have planned to make"
    },
    {
      "index": 111,
      "start_time": 736540.0,
      "end_time": 745769.0,
      "text": "a 132.8 EFLOPS supercomputer by 2017, 100 times faster than any supercomputer ever planned."
    },
    {
      "index": 112,
      "start_time": 745769.0,
      "end_time": 752220.0,
      "text": "They have estimated that the project would cost US $2 billion, which the state has budgeted."
    },
    {
      "index": 113,
      "start_time": 752220.0,
      "end_time": 755820.0,
      "text": "Cost of computing Hardware costs"
    },
    {
      "index": 114,
      "start_time": 755820.0,
      "end_time": 759519.0,
      "text": "The following is a list of examples of computers that demonstrates how drastically performance"
    },
    {
      "index": 115,
      "start_time": 759519.0,
      "end_time": 765889.0,
      "text": "has increased and price has decreased. The &quot;cost per GFLOPS&quot; is the cost for a set of"
    },
    {
      "index": 116,
      "start_time": 765889.0,
      "end_time": 770700.0,
      "text": "hardware that would theoretically operate at one billion floating-point operations per"
    },
    {
      "index": 117,
      "start_time": 770700.0,
      "end_time": 777420.0,
      "text": "second. During the era when no single computing platform was able to achieve one GFLOPS, this"
    },
    {
      "index": 118,
      "start_time": 777420.0,
      "end_time": 783500.0,
      "text": "table lists the total cost for multiple instances of a fast computing platform which speed sums"
    },
    {
      "index": 119,
      "start_time": 783500.0,
      "end_time": 791120.0,
      "text": "to one GFLOPS. Otherwise, the least expensive computing platform able to achieve one GFLOPS"
    },
    {
      "index": 120,
      "start_time": 791120.0,
      "end_time": 792760.0,
      "text": "is listed."
    },
    {
      "index": 121,
      "start_time": 792760.0,
      "end_time": 797740.0,
      "text": "The trend toward placing ever more transistors inexpensively on an integrated circuit follows"
    },
    {
      "index": 122,
      "start_time": 797740.0,
      "end_time": 805120.0,
      "text": "Moore&#39;s law. This trend explains the rising speed and falling cost of computer processing."
    },
    {
      "index": 123,
      "start_time": 805120.0,
      "end_time": 810279.0,
      "text": "Operation costs In energy cost, according to the Green500"
    },
    {
      "index": 124,
      "start_time": 810279.0,
      "end_time": 818440.0,
      "text": "list, as of June 2011 the most efficient TOP500 supercomputer runs at 2097.19 MFLOPS per watt."
    },
    {
      "index": 125,
      "start_time": 818440.0,
      "end_time": 825339.0,
      "text": "This translates to an energy requirement of 0.477 watts per GFLOPS, however this energy"
    },
    {
      "index": 126,
      "start_time": 825339.0,
      "end_time": 830399.0,
      "text": "requirement will be much greater for less efficient supercomputers."
    },
    {
      "index": 127,
      "start_time": 830399.0,
      "end_time": 834740.0,
      "text": "Hardware costs for low cost supercomputers may be less significant than energy costs"
    },
    {
      "index": 128,
      "start_time": 834740.0,
      "end_time": 840579.0,
      "text": "when running continuously for several years. Floating-point operation and integer operation"
    },
    {
      "index": 129,
      "start_time": 840579.0,
      "end_time": 847170.0,
      "text": "FLOPS measures the computing ability of a computer. An example of a floating-point operation"
    },
    {
      "index": 130,
      "start_time": 847170.0,
      "end_time": 854579.0,
      "text": "is the calculation of mathematical equations; as such, FLOPS is a useful measure of supercomputer"
    },
    {
      "index": 131,
      "start_time": 854579.0,
      "end_time": 860940.0,
      "text": "performance. MIPS is used to measure the integer performance of a computer. Examples of integer"
    },
    {
      "index": 132,
      "start_time": 860940.0,
      "end_time": 867589.0,
      "text": "operation include data movement or value testing. MIPS as a performance benchmark is adequate"
    },
    {
      "index": 133,
      "start_time": 867589.0,
      "end_time": 873350.0,
      "text": "for the computer when it is used in database query, word processing, spreadsheets, or to"
    },
    {
      "index": 134,
      "start_time": 873350.0,
      "end_time": 879730.0,
      "text": "run multiple virtual operating systems. Frank H. McMahon, of the Lawrence Livermore National"
    },
    {
      "index": 135,
      "start_time": 879730.0,
      "end_time": 885500.0,
      "text": "Laboratory, invented the terms FLOPS and MFLOPS so that he could compare the so-called supercomputers"
    },
    {
      "index": 136,
      "start_time": 885500.0,
      "end_time": 890440.0,
      "text": "of the day by the number of floating-point calculations they performed per second. This"
    },
    {
      "index": 137,
      "start_time": 890440.0,
      "end_time": 895899.0,
      "text": "was much better than using the prevalent MIPS to compare computers as this statistic usually"
    },
    {
      "index": 138,
      "start_time": 895899.0,
      "end_time": 900310.0,
      "text": "had little bearing on the arithmetic capability of the machine."
    },
    {
      "index": 139,
      "start_time": 900310.0,
      "end_time": 903820.0,
      "text": "Fixed-point These designations refer to the format used"
    },
    {
      "index": 140,
      "start_time": 903820.0,
      "end_time": 909850.0,
      "text": "to store and manipulate numeric representations of data without using a decimal point. Fixed-point"
    },
    {
      "index": 141,
      "start_time": 909850.0,
      "end_time": 916690.0,
      "text": "are designed to represent and manipulate integers – positive and negative whole numbers; for"
    },
    {
      "index": 142,
      "start_time": 916690.0,
      "end_time": 923959.0,
      "text": "example, 16 bits, yielding up to 65,536 possible bit patterns that typically represent the"
    },
    {
      "index": 143,
      "start_time": 923959.0,
      "end_time": 931889.0,
      "text": "whole numbers from −32768 to +32767. Floating-point"
    },
    {
      "index": 144,
      "start_time": 931889.0,
      "end_time": 936880.0,
      "text": "This is needed for very large or very small real numbers, or numbers requiring the use"
    },
    {
      "index": 145,
      "start_time": 936880.0,
      "end_time": 942290.0,
      "text": "of a decimal point. The encoding scheme used by the processor for floating-point numbers"
    },
    {
      "index": 146,
      "start_time": 942290.0,
      "end_time": 948110.0,
      "text": "is more complicated than for fixed-point. Floating-point representation is similar to"
    },
    {
      "index": 147,
      "start_time": 948110.0,
      "end_time": 952930.0,
      "text": "scientific notation, except everything is carried out in base two, rather than base"
    },
    {
      "index": 148,
      "start_time": 952930.0,
      "end_time": 959509.0,
      "text": "ten. The encoding scheme stores the sign, the exponent and the mantissa. While several"
    },
    {
      "index": 149,
      "start_time": 959509.0,
      "end_time": 968269.0,
      "text": "similar formats are in use, the most common is ANSI/IEEE Std. 754-1985. This standard"
    },
    {
      "index": 150,
      "start_time": 968269.0,
      "end_time": 974199.0,
      "text": "defines the format for 32-bit numbers called single precision, as well as 64-bit numbers"
    },
    {
      "index": 151,
      "start_time": 974199.0,
      "end_time": 979649.0,
      "text": "called double precision and longer numbers called extended precision. Floating-point"
    },
    {
      "index": 152,
      "start_time": 979649.0,
      "end_time": 984630.0,
      "text": "representations can support a much wider range of values than fixed-point, with the ability"
    },
    {
      "index": 153,
      "start_time": 984630.0,
      "end_time": 989139.0,
      "text": "to represent very small numbers and very large numbers."
    },
    {
      "index": 154,
      "start_time": 989139.0,
      "end_time": 993130.0,
      "text": "Dynamic range and precision The exponentiation inherent in floating-point"
    },
    {
      "index": 155,
      "start_time": 993130.0,
      "end_time": 998220.0,
      "text": "computation assures a much larger dynamic range – the largest and smallest numbers"
    },
    {
      "index": 156,
      "start_time": 998220.0,
      "end_time": 1003269.0,
      "text": "that can be represented – which is especially important when processing data sets which"
    },
    {
      "index": 157,
      "start_time": 1003270.0,
      "end_time": 1008780.0,
      "text": "are extremely large or where the range may be unpredictable. As such, floating-point"
    },
    {
      "index": 158,
      "start_time": 1008780.0,
      "end_time": 1013431.0,
      "text": "processors are ideally suited for computationally intensive applications."
    },
    {
      "index": 159,
      "start_time": 1013430.0,
      "end_time": 1015649.0,
      "text": "See also"
    },
    {
      "index": 160,
      "start_time": 1015650.0,
      "end_time": 1019001.0,
      "text": "Gordon Bell Prize Orders of magnitude"
    },
    {
      "index": 161,
      "start_time": 1019000.0,
      "end_time": 1020459.0,
      "text": "References"
    },
    {
      "index": 162,
      "start_time": 1020460.0,
      "end_time": 1024070.0,
      "text": "External links Current Einstein@Home benchmark"
    },
    {
      "index": 163,
      "start_time": 1024069.9999999999,
      "end_time": 1030160.9999999999,
      "text": "BOINC projects global benchmark Current GIMPS throughput"
    },
    {
      "index": 164,
      "start_time": 1030160.0000000001,
      "end_time": 1034380.0000000001,
      "text": "Top500.org LinuxHPC.org Linux High Performance Computing"
    },
    {
      "index": 165,
      "start_time": 1034380.0000000001,
      "end_time": 1039439.0000000001,
      "text": "and Clustering Portal WinHPC.org Windows High Performance Computing"
    },
    {
      "index": 166,
      "start_time": 1039440.0,
      "end_time": 1044471.0,
      "text": "and Clustering Portal Oscar Linux-cluster ranking list by CPUs/types"
    },
    {
      "index": 167,
      "start_time": 1044470.0,
      "end_time": 1049110.0,
      "text": "and respective FLOPS Information on how to calculate &quot;Composite"
    },
    {
      "index": 168,
      "start_time": 1049110.0,
      "end_time": 1052669.0,
      "text": "Theoretical Performance&quot; Information on the Oak Ridge National Laboratory"
    },
    {
      "index": 169,
      "start_time": 1052670.0,
      "end_time": 1058761.0,
      "text": "Cray XT system. Infiscale Cluster Portal – Free GPL HPC"
    },
    {
      "index": 170,
      "start_time": 1058760.0,
      "end_time": 1065220.0,
      "text": "Source code, pre-compiled versions and results for PCs – Linpack, Livermore Loops, Whetstone"
    },
    {
      "index": 171,
      "start_time": 1065220.0,
      "end_time": 1069300.0,
      "text": "MFLOPS PC CPU Performance Comparisons %MFLOPS/MHz"
    },
    {
      "index": 172,
      "start_time": 1069300.0,
      "end_time": 1076090.0,
      "text": "– CPU, Caches and RAM Xeon export compliance metrics, including"
    },
    {
      "index": 173,
      "start_time": 1076090.0,
      "end_time": 1086090.0,
      "text": "GFLOPS IBM Brings NVIDIA Tesla GPUs Onboard"
    }
  ]
}